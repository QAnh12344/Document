import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader
from torch.utils.tensorboard import SummaryWriter
from captum.attr import IntegratedGradients
import matplotlib.pyplot as plt
import numpy as np
import random, os

# -----------------------------
# 1Ô∏è‚É£ SETUP
# -----------------------------
torch.manual_seed(42)
random.seed(42)
np.random.seed(42)

input_dim = 20
hidden_dim = 32
num_classes = 3
num_samples = 500
batch_size = 32
epochs = 15
log_dir = "runs/model_interpret_full"

os.makedirs(log_dir, exist_ok=True)
writer = SummaryWriter(log_dir=log_dir)

# -----------------------------
# 2Ô∏è‚É£ DATA: random cho d·ªÖ nh√¨n
# -----------------------------
X = torch.randn(num_samples, 10, input_dim)  # (batch, seq, features)
y = torch.randint(0, num_classes, (num_samples,))

dataset = TensorDataset(X, y)
loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

# -----------------------------
# 3Ô∏è‚É£ MODEL: LSTM c∆° b·∫£n
# -----------------------------
class InterpretableLSTM(nn.Module):
    def __init__(self, input_dim, hidden_dim, num_classes):
        super().__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, num_classes)
        self.relu = nn.ReLU()

    def forward(self, x):
        h, _ = self.lstm(x)
        out = self.relu(h[:, -1, :])  # l·∫•y hidden cu·ªëi
        logits = self.fc(out)
        return logits, {"hidden": h, "activation": out}

model = InterpretableLSTM(input_dim, hidden_dim, num_classes)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=1e-3)

# -----------------------------
# 4Ô∏è‚É£ TRAIN LOOP + LOG
# -----------------------------
for epoch in range(epochs):
    model.train()
    total_loss, correct = 0, 0

    for batch_X, batch_y in loader:
        optimizer.zero_grad()
        logits, internals = model(batch_X)
        loss = criterion(logits, batch_y)
        loss.backward()

        optimizer.step()

        total_loss += loss.item()
        correct += (logits.argmax(1) == batch_y).sum().item()

        # Log histograms
        for name, param in model.named_parameters():
            writer.add_histogram(f"weights/{name}", param, epoch)
            if param.grad is not None:
                writer.add_histogram(f"grads/{name}", param.grad, epoch)

        # Log activation stats
        writer.add_histogram("activations/last_hidden", internals["activation"], epoch)

    acc = correct / len(dataset)
    avg_loss = total_loss / len(loader)
    writer.add_scalar("Loss/train", avg_loss, epoch)
    writer.add_scalar("Accuracy/train", acc, epoch)

    print(f"Epoch {epoch+1}/{epochs} | Loss: {avg_loss:.4f} | Acc: {acc:.3f}")

# -----------------------------
# 5Ô∏è‚É£ EMBEDDING PROJECTOR
# -----------------------------
embeddings = []
labels = []
with torch.no_grad():
    for Xb, yb in loader:
        logits, internals = model(Xb)
        embeddings.append(internals["activation"])
        labels.append(yb)
embeddings = torch.cat(embeddings)
labels = torch.cat(labels)
writer.add_embedding(embeddings, metadata=labels, tag="embedding_space")

# -----------------------------
# 6Ô∏è‚É£ FEATURE IMPORTANCE (Captum)
# -----------------------------
ig = IntegratedGradients(model)
model.eval()
sample_X = X[0:1]
sample_y = y[0:1]
attr, delta = ig.attribute(sample_X, target=sample_y.item(), return_convergence_delta=True)

# V·∫Ω heatmap th·ªÉ hi·ªán importance theo chi·ªÅu th·ªùi gian + feature
attr_np = attr.squeeze(0).detach().numpy()
plt.figure(figsize=(8, 4))
plt.imshow(attr_np.T, cmap="hot", aspect="auto")
plt.colorbar(label="Feature importance")
plt.title("Integrated Gradients Heatmap (Captum)")
plt.xlabel("Time step")
plt.ylabel("Feature dim")
plt.tight_layout()
plt.savefig(os.path.join(log_dir, "feature_importance_heatmap.png"))
plt.close()

writer.close()
print("‚úÖ Training done. Logs saved to:", log_dir)
print("Run TensorBoard: tensorboard --logdir runs/model_interpret_full")









import torch
import matplotlib.pyplot as plt

# ============================================================
# 1Ô∏è‚É£ T·∫°o ma tr·∫≠n HiPPO-LegS (A, B)
# ============================================================
def hippo_legs_matrix(N: int):
    """T·∫°o ma tr·∫≠n A, B cho HiPPO-LegS"""
    n = torch.arange(N, dtype=torch.float32)
    A = torch.zeros((N, N))
    for i in range(N):
        for j in range(N):
            if j <= i:
                A[i, j] = -(2 * i + 1) * ((-1) ** (i - j))
    B = (2 * n + 1).sqrt().unsqueeze(-1)
    return A, B

# ============================================================
# 2Ô∏è‚É£ R·ªùi r·∫°c h√≥a b·∫±ng Bilinear transform (Tustin)
# ============================================================
def bilinear_discretize(A, B, dt=0.01):
    I = torch.eye(A.size(0))
    Ad = (I + 0.5 * dt * A) @ torch.linalg.inv(I - 0.5 * dt * A)
    Bd = torch.linalg.inv(I - 0.5 * dt * A) @ B * dt
    return Ad, Bd

# ============================================================
# 3Ô∏è‚É£ M√¥ ph·ªèng HiPPO v·ªõi t√≠n hi·ªáu x(t) = t
# ============================================================
def simulate_hippo(N=4, T=1.0, dt=0.01):
    steps = int(T / dt)
    A, B = hippo_legs_matrix(N)
    Ad, Bd = bilinear_discretize(A, B, dt)

    c = torch.zeros(N, 1)
    states = [c.clone()]
    xs = []
    ts = []

    for step in range(steps):
        t = step * dt
        x_t = torch.tensor([t])  # x(t) = t
        c = Ad @ c + Bd * x_t
        states.append(c.clone())
        xs.append(x_t.item())
        ts.append(t)

    states = torch.cat(states, dim=1)
    return ts, xs, states

# ============================================================
# 4Ô∏è‚É£ V·∫Ω k·∫øt qu·∫£
# ============================================================
ts, xs, states = simulate_hippo(N=4, T=1.0, dt=0.01)

plt.figure(figsize=(10, 6))
for i in range(states.size(0)):
    plt.plot(ts, states[i, 1:].numpy(), label=f'c[{i}]')

plt.title("Ti·∫øn h√≥a h·ªá s·ªë HiPPO-LegS cho x(t)=t")
plt.xlabel("Th·ªùi gian t")
plt.ylabel("Gi√° tr·ªã h·ªá s·ªë c_i(t)")
plt.legend()
plt.grid(True)
plt.show()





import pandas as pd
import numpy as np

INPUT = "NF-ToN-IoT-v2_1M_balanced.csv"
OUTPUT = "NF-ToN-IoT-v2_1M_balanced_shuffled.csv"
CHUNK_SIZE = 100_000
RANDOM_STATE = 1234

# 1. ƒê·ªçc file th√†nh t·ª´ng kh·ªëi
chunks = pd.read_csv(INPUT, chunksize=CHUNK_SIZE)

# 2. Shuffle th·ª© t·ª± c√°c kh·ªëi
chunks = list(chunks)
np.random.seed(RANDOM_STATE)
np.random.shuffle(chunks)

# 3. Shuffle trong t·ª´ng kh·ªëi (√≠t t·ªën RAM)
for i, chunk in enumerate(chunks):
    chunk = chunk.sample(frac=1.0, random_state=RANDOM_STATE + i)
    mode = "w" if i == 0 else "a"
    header = (i == 0)
    chunk.to_csv(OUTPUT, index=False, mode=mode, header=header)

print("‚úÖ ƒê√£ shuffle to√†n b·ªô v√† l∆∞u:", OUTPUT)




import pandas as pd
import numpy as np

# ==============================================================
# ‚öôÔ∏è C·∫§U H√åNH
# ==============================================================
INPUT = "NF-ToN-IoT-v2_100k.csv"           # file ƒë·∫ßu v√†o
OUTPUT = "NF-ToN-IoT-v2_100k_clean.csv"    # file sau khi l√†m s·∫°ch
CLIP_MIN, CLIP_MAX = -1e6, 1e6             # gi·ªõi h·∫°n gi√° tr·ªã c·ª±c tr·ªã

# ==============================================================
# 1Ô∏è‚É£ ƒê·ªçc d·ªØ li·ªáu
# ==============================================================
df = pd.read_csv(INPUT)
print("üìä K√≠ch th∆∞·ªõc ban ƒë·∫ßu:", df.shape)

if 'Attack' not in df.columns:
    raise ValueError("‚ùå Kh√¥ng t√¨m th·∫•y c·ªôt 'Attack' trong file CSV.")

# ==============================================================
# 2Ô∏è‚É£ X·ª≠ l√Ω NaN
# ==============================================================
# Xo√° d√≤ng kh√¥ng c√≥ nh√£n Attack
df = df.dropna(subset=['Attack']).reset_index(drop=True)

# X√°c ƒë·ªãnh c·ªôt s·ªë v√† c·ªôt chu·ªói
num_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()
cat_cols = df.select_dtypes(include=['object']).columns.drop('Attack').tolist()

# ƒêi·ªÅn gi√° tr·ªã thi·∫øu
for col in num_cols:
    df[col] = df[col].fillna(df[col].median())

for col in cat_cols:
    df[col] = df[col].fillna(df[col].mode()[0])

print(f"‚úÖ ƒê√£ x·ª≠ l√Ω NaN: {len(num_cols)} c·ªôt s·ªë, {len(cat_cols)} c·ªôt chu·ªói.")

# ==============================================================
# 3Ô∏è‚É£ Gi·ªõi h·∫°n (clip) gi√° tr·ªã c·ª±c tr·ªã
# ==============================================================
df[num_cols] = df[num_cols].clip(lower=CLIP_MIN, upper=CLIP_MAX)
print(f"‚úÖ ƒê√£ gi·ªõi h·∫°n gi√° tr·ªã trong kho·∫£ng [{CLIP_MIN}, {CLIP_MAX}]")

# ==============================================================
# 4Ô∏è‚É£ L∆∞u d·ªØ li·ªáu s·∫°ch
# ==============================================================
df.to_csv(OUTPUT, index=False)
print("‚úÖ ƒê√£ l∆∞u file l√†m s·∫°ch:", OUTPUT)





import pandas as pd
from sklearn.utils import resample

INPUT = "NF-ToN-IoT-v2.csv"
OUTPUT = "NF-ToN-IoT-v2_1M_balanced.csv"
TARGET_TOTAL = 1_000_000
RANDOM_STATE = 1234

df = pd.read_csv(INPUT)

if 'Attack' not in df.columns:
    raise ValueError("File CSV kh√¥ng c√≥ c·ªôt 'Attack'")

classes = df['Attack'].unique()
num_classes = len(classes)
target_per_class = TARGET_TOTAL // num_classes
remainder = TARGET_TOTAL % num_classes

sampled_parts = []
for i, cls in enumerate(sorted(classes)):
    cls_df = df[df['Attack'] == cls]
    this_target = target_per_class + (1 if i < remainder else 0)
    part = cls_df.sample(
        n=this_target,
        replace=(len(cls_df) < this_target),
        random_state=RANDOM_STATE,
    ).sample(frac=1.0, random_state=RANDOM_STATE)  # shuffle ri√™ng t·ª´ng l·ªõp
    sampled_parts.append(part)

# concat m√† kh√¥ng shuffle to√†n c·ª•c
df_balanced = pd.concat(sampled_parts, ignore_index=True)

print(df_balanced['Attack'].value_counts())

df_balanced.to_csv(OUTPUT, index=False)
print("Saved to:", OUTPUT)





