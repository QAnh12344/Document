# =====================================================
# VIETNAMESE CONTINUED PRETRAINING FOR MAMBA-130M
# WITH TRAIN + VALID, DDP, AMP, EXPLICIT LOSS
# =====================================================

import os
import math
import torch
import torch.distributed as dist
import torch.nn.functional as F

from torch.utils.data import Dataset, DataLoader, DistributedSampler
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.nn.utils.rnn import pad_sequence

from transformers import AutoTokenizer
from mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel

# ---------------- CONFIG ----------------
TEXT_PATH = "corpus_vi.txt"   # text tiáº¿ng Viá»‡t
SEQ_LEN = 2048
BATCH_SIZE = 2               # per GPU
EPOCHS = 1
LR = 2e-5                    # an toÃ n cho continued pretrain
GRAD_CLIP = 0.5
VALID_RATIO = 0.05

# ---------------- DDP INIT ----------------
dist.init_process_group("nccl")
local_rank = int(os.environ["LOCAL_RANK"])
torch.cuda.set_device(local_rank)
device = torch.device("cuda", local_rank)

torch.backends.cuda.matmul.allow_tf32 = True

# ---------------- TOKENIZER ----------------
tokenizer = AutoTokenizer.from_pretrained("EleutherAI/gpt-neox-20b")
tokenizer.pad_token = tokenizer.eos_token
PAD_ID = tokenizer.pad_token_id

# ---------------- DATASET ----------------
class LMDataset(Dataset):
    def __init__(self, text, tokenizer, seq_len):
        ids = tokenizer.encode(text)
        self.samples = []
        for i in range(0, len(ids) - seq_len, seq_len):
            self.samples.append(
                torch.tensor(ids[i:i + seq_len + 1], dtype=torch.long)
            )

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        return self.samples[idx]

def collate_fn(batch):
    batch = pad_sequence(batch, batch_first=True, padding_value=PAD_ID)

    input_ids = batch[:, :-1]
    labels = batch[:, 1:].clone()
    labels[labels == PAD_ID] = -100

    return input_ids, labels

# ---------------- LOAD DATA ----------------
with open(TEXT_PATH, encoding="utf8") as f:
    text = f.read()

split = int(len(text) * (1 - VALID_RATIO))
train_text = text[:split]
valid_text = text[split:]

train_ds = LMDataset(train_text, tokenizer, SEQ_LEN)
valid_ds = LMDataset(valid_text, tokenizer, SEQ_LEN)

train_sampler = DistributedSampler(train_ds, shuffle=True)
valid_sampler = DistributedSampler(valid_ds, shuffle=False)

train_loader = DataLoader(
    train_ds,
    batch_size=BATCH_SIZE,
    sampler=train_sampler,
    collate_fn=collate_fn,
    num_workers=2,
    pin_memory=True
)

valid_loader = DataLoader(
    valid_ds,
    batch_size=BATCH_SIZE,
    sampler=valid_sampler,
    collate_fn=collate_fn,
    num_workers=2,
    pin_memory=True
)

# ---------------- MODEL ----------------
model = MambaLMHeadModel.from_pretrained(
    "state-spaces/mamba-130m",
    device=device,
    dtype=torch.float16
)

# resize cho cháº¯c cháº¯n tokenizer â†” model khá»›p
model.resize_token_embeddings(len(tokenizer))
model.train()

model = DDP(model, device_ids=[local_rank])

# ---------------- OPTIMIZER + AMP ----------------
optimizer = torch.optim.AdamW(
    model.parameters(),
    lr=LR,
    betas=(0.9, 0.95),
    weight_decay=0.1
)

scaler = torch.cuda.amp.GradScaler()

# ---------------- VALIDATION ----------------
@torch.no_grad()
def evaluate():
    model.eval()
    total_loss = 0.0
    total_tokens = 0

    for input_ids, labels in valid_loader:
        input_ids = input_ids.to(device, non_blocking=True)
        labels = labels.to(device, non_blocking=True)

        logits = model.module(input_ids).logits
        vocab = logits.size(-1)

        loss = F.cross_entropy(
            logits.reshape(-1, vocab),
            labels.reshape(-1),
            ignore_index=-100,
            reduction="sum"
        )

        total_loss += loss.item()
        total_tokens += (labels != -100).sum().item()

    model.train()
    return total_loss / total_tokens

# ---------------- TRAIN LOOP ----------------
for epoch in range(EPOCHS):
    train_sampler.set_epoch(epoch)

    for step, (input_ids, labels) in enumerate(train_loader):
        input_ids = input_ids.to(device, non_blocking=True)
        labels = labels.to(device, non_blocking=True)

        # trÃ¡nh batch toÃ n -100
        if (labels != -100).sum() == 0:
            continue

        with torch.cuda.amp.autocast():
            logits = model(input_ids).logits
            vocab = logits.size(-1)

            loss = F.cross_entropy(
                logits.reshape(-1, vocab),
                labels.reshape(-1),
                ignore_index=-100
            )

        scaler.scale(loss).backward()
        scaler.unscale_(optimizer)
        torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)

        scaler.step(optimizer)
        scaler.update()
        optimizer.zero_grad(set_to_none=True)

        if step % 50 == 0 and local_rank == 0:
            print(f"[Epoch {epoch} | Step {step}] Train loss: {loss.item():.4f}")

        if step % 500 == 0 and step > 0 and local_rank == 0:
            val_loss = evaluate()
            ppl = math.exp(val_loss)
            print(f"[VALID] loss={val_loss:.4f} | ppl={ppl:.2f}")

# ---------------- SAVE ----------------
if local_rank == 0:
    model.module.save_pretrained("vimamba-130m-vi")
    tokenizer.save_pretrained("vimamba-130m-vi")

dist.destroy_process_group()









# ============================================
# MAMBA SSM â€“ VIETNAMESE CONTINUED PRETRAINING
# WITH EXPLICIT LABEL & LOSS (DDP)
# ============================================

import os
import torch
import torch.distributed as dist
import torch.nn.functional as F

from torch.utils.data import Dataset, DataLoader, DistributedSampler
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.nn.utils.rnn import pad_sequence

from transformers import AutoTokenizer
from mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel

# ---------------- CONFIG ----------------
TEXT_PATH = "corpus_vi.txt"
SEQ_LEN = 2048
BATCH_SIZE = 2        # per GPU
EPOCHS = 1
LR = 1e-4

# ---------------- DDP INIT ----------------
dist.init_process_group("nccl")
local_rank = int(os.environ["LOCAL_RANK"])
torch.cuda.set_device(local_rank)
device = torch.device("cuda", local_rank)

# ---------------- TOKENIZER ----------------
tokenizer = AutoTokenizer.from_pretrained("EleutherAI/gpt-neox-20b")
tokenizer.pad_token = tokenizer.eos_token
PAD_ID = tokenizer.pad_token_id
VOCAB_SIZE = tokenizer.vocab_size

# ---------------- DATASET ----------------
class VietnameseLMDataset(Dataset):
    def __init__(self, path, tokenizer, seq_len):
        with open(path, "r", encoding="utf8") as f:
            text = f.read()

        ids = tokenizer.encode(text)
        self.samples = []

        for i in range(0, len(ids) - seq_len, seq_len):
            # +1 Ä‘á»ƒ shift label
            self.samples.append(
                torch.tensor(ids[i:i + seq_len + 1], dtype=torch.long)
            )

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        return self.samples[idx]

def collate_fn(batch):
    # batch: List[(T+1)]
    batch = pad_sequence(batch, batch_first=True, padding_value=PAD_ID)

    # x0..x(n-1)
    input_ids = batch[:, :-1]

    # x1..xn
    labels = batch[:, 1:].clone()
    labels[labels == PAD_ID] = -100

    return input_ids, labels

# ---------------- LOAD MODEL ----------------
model = MambaLMHeadModel.from_pretrained(
    "state-spaces/mamba-130m",
    device=device,
    dtype=torch.float16
)
model.train()

model = DDP(model, device_ids=[local_rank])

# ---------------- DATALOADER ----------------
dataset = VietnameseLMDataset(TEXT_PATH, tokenizer, SEQ_LEN)
sampler = DistributedSampler(dataset, shuffle=True)

loader = DataLoader(
    dataset,
    batch_size=BATCH_SIZE,
    sampler=sampler,
    collate_fn=collate_fn,
    num_workers=2,
    pin_memory=True
)

# ---------------- OPTIMIZER ----------------
optimizer = torch.optim.AdamW(
    model.parameters(),
    lr=LR,
    betas=(0.9, 0.95),
    weight_decay=0.1
)

# ---------------- TRAIN LOOP ----------------
for epoch in range(EPOCHS):
    sampler.set_epoch(epoch)

    for step, (input_ids, labels) in enumerate(loader):
        input_ids = input_ids.to(device, non_blocking=True)
        labels = labels.to(device, non_blocking=True)

        # ---- FORWARD (NO LOSS INSIDE MODEL) ----
        outputs = model.module(input_ids=input_ids)
        logits = outputs.logits        # (B, T, V)

        # ---- EXPLICIT LOSS ----
        loss = F.cross_entropy(
            logits.view(-1, VOCAB_SIZE),
            labels.view(-1),
            ignore_index=-100
        )

        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()
        optimizer.zero_grad()

        if step % 50 == 0 and local_rank == 0:
            print(
                f"[Epoch {epoch} | Step {step}] "
                f"Loss: {loss.item():.4f}"
            )

# ---------------- SAVE ----------------
if local_rank == 0:
    model.module.save_pretrained("vimamba-130m-vi")
    tokenizer.save_pretrained("vimamba-130m-vi")

dist.destroy_process_group()











# ============================
# DISTRIBUTED VIETNAMESE CONTINUED PRETRAINING FOR MAMBA
# ============================

import os
import torch
import torch.distributed as dist
from torch.utils.data import Dataset, DataLoader, DistributedSampler
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.nn.utils.rnn import pad_sequence

from transformers import AutoTokenizer
from mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel

# -------- CONFIG --------
TEXT_PATH = "corpus_vi.txt"
SEQ_LEN = 2048
BATCH_SIZE = 2          # per GPU
EPOCHS = 1
LR = 1e-4

# -------- DDP INIT --------
dist.init_process_group("nccl")
local_rank = int(os.environ["LOCAL_RANK"])
torch.cuda.set_device(local_rank)
device = torch.device("cuda", local_rank)

# -------- TOKENIZER --------
tokenizer = AutoTokenizer.from_pretrained("EleutherAI/gpt-neox-20b")
tokenizer.pad_token = tokenizer.eos_token
PAD_ID = tokenizer.pad_token_id

# -------- DATASET --------
class VietnameseLMDataset(Dataset):
    def __init__(self, path, tokenizer, seq_len):
        with open(path, "r", encoding="utf8") as f:
            text = f.read()

        ids = tokenizer.encode(text)
        self.samples = []
        for i in range(0, len(ids) - seq_len, seq_len):
            self.samples.append(
                torch.tensor(ids[i:i + seq_len + 1], dtype=torch.long)
            )

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        return self.samples[idx]

def collate_fn(batch):
    batch = pad_sequence(batch, batch_first=True, padding_value=PAD_ID)
    input_ids = batch[:, :-1]
    labels = batch[:, 1:].clone()
    labels[labels == PAD_ID] = -100
    return input_ids, labels

# -------- LOAD MODEL --------
model = MambaLMHeadModel.from_pretrained(
    "state-spaces/mamba-130m",
    device=device,
    dtype=torch.float16
)
model.train()

model = DDP(model, device_ids=[local_rank], output_device=local_rank)

# -------- DATALOADER --------
dataset = VietnameseLMDataset(TEXT_PATH, tokenizer, SEQ_LEN)
sampler = DistributedSampler(dataset, shuffle=True)

loader = DataLoader(
    dataset,
    batch_size=BATCH_SIZE,
    sampler=sampler,
    collate_fn=collate_fn,
    num_workers=2,
    pin_memory=True
)

# -------- OPTIMIZER --------
optimizer = torch.optim.AdamW(
    model.parameters(),
    lr=LR,
    betas=(0.9, 0.95),
    weight_decay=0.1
)

# -------- TRAIN LOOP --------
for epoch in range(EPOCHS):
    sampler.set_epoch(epoch)

    for step, (input_ids, labels) in enumerate(loader):
        input_ids = input_ids.to(device, non_blocking=True)
        labels = labels.to(device, non_blocking=True)

        out = model(input_ids=input_ids, labels=labels)
        loss = out.loss

        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()
        optimizer.zero_grad()

        if step % 50 == 0 and local_rank == 0:
            print(f"[Epoch {epoch} | Step {step}] Loss: {loss.item():.4f}")

# -------- SAVE (only rank 0) --------
if local_rank == 0:
    model.module.save_pretrained("vimamba-130m-vi")
    tokenizer.save_pretrained("vimamba-130m-vi")

dist.destroy_process_group()









import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import random

# ===================== FAKE DATA =====================
def fake_data(num_samples=100):
    mels, tokens = [], []
    for _ in range(num_samples):
        T_mel = random.randint(200, 600)
        T_tok = random.randint(10, 50)
        mels.append(torch.randn(T_mel, 80))            # (T_mel, 80)
        tokens.append(torch.randint(1, 100, (T_tok,))) # vocab=100, 0=PAD
    return mels, tokens

# ===================== DATASET =====================
class SpeechDataset(Dataset):
    def __init__(self, mels, tokens):
        self.mels = mels
        self.tokens = tokens

    def __len__(self):
        return len(self.mels)

    def __getitem__(self, idx):
        return self.mels[idx], self.tokens[idx]

# ===================== COLLATE =====================
def collate_fn(batch):
    mels, tokens = zip(*batch)
    B = len(batch)

    T_mel_max = max(m.shape[0] for m in mels)
    T_tok_max = max(len(t) for t in tokens)

    mel_batch = torch.zeros(B, T_mel_max, 80)
    mel_mask  = torch.zeros(B, T_mel_max)
    tok_batch = torch.zeros(B, T_tok_max, dtype=torch.long)
    tok_mask  = torch.zeros(B, T_tok_max)

    for i, (m, t) in enumerate(zip(mels, tokens)):
        mel_batch[i, :m.shape[0]] = m
        mel_mask[i, :m.shape[0]] = 1
        tok_batch[i, :len(t)] = t
        tok_mask[i, :len(t)] = 1

    return mel_batch, tok_batch, mel_mask, tok_mask

# ===================== MODEL =====================
class SimpleASR(nn.Module):
    def __init__(self):
        super().__init__()
        self.lstm = nn.LSTM(80, 256, batch_first=True, bidirectional=True)
        self.fc = nn.Linear(512, 100)

    def forward(self, x):
        x, _ = self.lstm(x)
        return self.fc(x)

# ===================== MAIN =====================
def main():
    device = "cuda" if torch.cuda.is_available() else "cpu"

    mels, tokens = fake_data(200)
    dataset = SpeechDataset(mels, tokens)
    loader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)

    model = SimpleASR().to(device)
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)
    criterion = nn.CrossEntropyLoss(reduction="none")

    for epoch in range(5):
        total_loss = 0
        for mel, tok, mel_mask, tok_mask in loader:
            mel, tok, tok_mask = mel.to(device), tok.to(device), tok_mask.to(device)

            logits = model(mel)               # (B, T_mel, vocab)
            B, T, V = logits.shape

            tok = tok[:, :T]
            tok_mask = tok_mask[:, :T]

            loss = criterion(
                logits.reshape(B*T, V),
                tok.reshape(B*T)
            )
            mask = tok_mask.reshape(B*T)
            loss = (loss * mask).sum() / mask.sum()

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        print(f"Epoch {epoch} | loss = {total_loss:.4f}")

    mel, tok, mel_mask, tok_mask = next(iter(loader))
    print(mel.shape, tok.shape, mel_mask.shape, tok_mask.shape)

if __name__ == "__main__":
    main()






import os
import torch
import pandas as pd
import numpy as np
from torch import nn, optim
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer
from mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel
from sklearn.metrics import (
    accuracy_score, f1_score, classification_report,
    precision_recall_fscore_support, confusion_matrix
)
from tqdm import tqdm
import seaborn as sns
import matplotlib.pyplot as plt

# ======================================================
# 1. CONFIG
# ======================================================
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
MODEL_NAME = "state-spaces/mamba-130m"
MAX_LEN = 128
BATCH_SIZE = 32
LR = 3e-6
EPOCHS = 10
NUM_CLASSES = 3

# ======================================================
# 2. LOAD DATA
# ======================================================
def load_vsfc_data(folder):
    with open(os.path.join(folder, "sents.txt"), encoding="utf-8") as f:
        texts = [l.strip() for l in f]
    with open(os.path.join(folder, "sentiments.txt")) as f:
        labels = [int(l.strip()) for l in f]
    return pd.DataFrame({"text": texts, "label": labels})

tokenizer = AutoTokenizer.from_pretrained("EleutherAI/gpt-neox-20b")
tokenizer.pad_token = tokenizer.eos_token

class VSFCDataset(Dataset):
    def __init__(self, df):
        self.texts = df.text.values
        self.labels = df.label.values

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        enc = tokenizer(
            self.texts[idx],
            max_length=MAX_LEN,
            padding="max_length",
            truncation=True,
            return_tensors="pt"
        )
        return {
            "input_ids": enc["input_ids"].squeeze(0),
            "labels": torch.tensor(self.labels[idx], dtype=torch.long)
        }

df_train = load_vsfc_data("train")
df_dev   = load_vsfc_data("dev")
df_test  = load_vsfc_data("test")

train_loader = DataLoader(VSFCDataset(df_train), BATCH_SIZE, shuffle=True)
dev_loader   = DataLoader(VSFCDataset(df_dev), BATCH_SIZE)
test_loader  = DataLoader(VSFCDataset(df_test), BATCH_SIZE)

# ======================================================
# 3. MODEL
# ======================================================
class MambaSupCon(nn.Module):
    def __init__(self, model_name):
        super().__init__()
        self.mamba = MambaLMHeadModel.from_pretrained(model_name)
        d_model = self.mamba.config.d_model

        self.dropout = nn.Dropout(0.1)
        self.classifier = nn.Linear(d_model, NUM_CLASSES)
        self.projector = nn.Sequential(
            nn.Linear(d_model, 256),
            nn.ReLU(),
            nn.Linear(256, 128)
        )

    def forward(self, input_ids):
        hidden = self.mamba.backbone(input_ids)  # (B, L, D)

        # mask-aware mean pooling
        mask = (input_ids != tokenizer.pad_token_id).unsqueeze(-1)
        hidden = hidden * mask
        pooled = hidden.sum(1) / mask.sum(1).clamp(min=1)

        logits = self.classifier(self.dropout(pooled))
        proj   = self.projector(pooled)
        return logits, proj

# ======================================================
# 4. SAFE SUPCON LOSS
# ======================================================
class SupConLoss(nn.Module):
    def __init__(self, temperature=0.1):
        super().__init__()
        self.temp = temperature

    def forward(self, feats, labels):
        feats = nn.functional.normalize(feats, dim=1)
        device = feats.device
        bsz = feats.size(0)

        labels = labels.view(-1, 1)
        mask = torch.eq(labels, labels.T).float().to(device)

        logits = torch.matmul(feats, feats.T) / self.temp
        logits = logits - logits.max(dim=1, keepdim=True)[0].detach()

        eye = torch.eye(bsz, device=device)
        logits_mask = 1 - eye
        mask = mask * logits_mask

        exp_logits = torch.exp(logits) * logits_mask
        log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True) + 1e-12)

        denom = mask.sum(1)
        valid = denom > 0
        if valid.sum() == 0:
            return torch.tensor(0.0, device=device, requires_grad=True)

        loss = -(mask * log_prob).sum(1)[valid] / denom[valid]
        return loss.mean()

# ======================================================
# 5. TRAINING
# ======================================================
model = MambaSupCon(MODEL_NAME).to(DEVICE)

# freeze backbone first (OPTIONAL but stable)
for p in model.mamba.parameters():
    p.requires_grad = False

optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=LR)

class_weights = torch.tensor([1.0, 8.0, 1.0]).to(DEVICE)
criterion_cls = nn.CrossEntropyLoss(weight=class_weights)
criterion_con = SupConLoss()

best_f1 = 0
print("ðŸš€ Start training...")

for epoch in range(EPOCHS):
    model.train()
    total_loss = 0

    for batch in tqdm(train_loader, desc=f"Epoch {epoch+1}"):
        ids = batch["input_ids"].to(DEVICE)
        labels = batch["labels"].to(DEVICE)

        optimizer.zero_grad()
        logits, proj = model(ids)

        loss_cls = criterion_cls(logits, labels)
        loss_con = criterion_con(proj, labels)
        loss = 0.7 * loss_cls + 0.3 * loss_con

        if torch.isnan(loss):
            print("âš ï¸ NaN skipped")
            continue

        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    # validation
    model.eval()
    y_true, y_pred = [], []
    with torch.no_grad():
        for b in dev_loader:
            l, _ = model(b["input_ids"].to(DEVICE))
            y_pred.extend(torch.argmax(l, 1).cpu().numpy())
            y_true.extend(b["labels"].numpy())

    f1 = f1_score(y_true, y_pred, average="macro")
    print(f"Loss: {total_loss/len(train_loader):.4f} | Val F1: {f1:.4f}")

    if f1 > best_f1:
        best_f1 = f1
        torch.save(model.state_dict(), "best_mamba_supcon.pt")

# ======================================================
# 6. TEST
# ======================================================
model.load_state_dict(torch.load("best_mamba_supcon.pt"))
model.eval()

preds = []
with torch.no_grad():
    for b in test_loader:
        l, _ = model(b["input_ids"].to(DEVICE))
        preds.extend(torch.argmax(l, 1).cpu().numpy())

y_true = df_test.label.values
acc = accuracy_score(y_true, preds)
p, r, f1, _ = precision_recall_fscore_support(y_true, preds, average="macro")

print("="*60)
print(f"Accuracy: {acc*100:.2f}%")
print(f"Macro P/R/F1: {p*100:.2f} / {r*100:.2f} / {f1*100:.2f}")
print("="*60)
print(classification_report(y_true, preds, target_names=["Neg", "Neu", "Pos"]))

cm = confusion_matrix(y_true, preds)
sns.heatmap(cm, annot=True, fmt="d", cmap="Oranges",
            xticklabels=["Neg", "Neu", "Pos"],
            yticklabels=["Neg", "Neu", "Pos"])
plt.show()







import matplotlib.pyplot as plt
import numpy as np

# Models on x-axis
models = ["PhoBERT", "RoBERTa", "Mamba"]

# Mean scores (%)
means = {
    "Accuracy":  [92.8, 91.9, 93.4],
    "Precision": [93.1, 92.3, 93.6],
    "Recall":    [92.4, 91.5, 93.1],
    "F1":        [92.7, 91.8, 93.3],
}

# Standard deviation (vÃ­ dá»¥ tá»« multiple runs)
stds = {
    "Accuracy":  [0.3, 0.4, 0.2],
    "Precision": [0.2, 0.3, 0.2],
    "Recall":    [0.4, 0.5, 0.3],
    "F1":        [0.3, 0.4, 0.2],
}

x = np.arange(len(models))
width = 0.2

plt.figure()

plt.bar(x - 1.5*width, means["Accuracy"],  width,
        yerr=stds["Accuracy"], capsize=5, label="Accuracy")
plt.bar(x - 0.5*width, means["Precision"], width,
        yerr=stds["Precision"], capsize=5, label="Precision")
plt.bar(x + 0.5*width, means["Recall"],    width,
        yerr=stds["Recall"], capsize=5, label="Recall")
plt.bar(x + 1.5*width, means["F1"],        width,
        yerr=stds["F1"], capsize=5, label="F1")

plt.xlabel("Models")
plt.ylabel("Score (%)")
plt.title("Model Comparison with Error Bars (Mean Â± Std)")
plt.xticks(x, models)
plt.legend()

plt.tight_layout()
plt.show()










import os
import time
import librosa
import torch
import onnxruntime as ort

from optimum.onnxruntime import ORTModelForSpeechSeq2Seq
from transformers import WhisperProcessor


# =====================
# CONFIG
# =====================
ONNX_DIR = "./whisper_small_onnx"
WAV_SCP = "./wav.scp"

LANGUAGE = "vi"
TASK = "transcribe"

MAX_NEW_TOKENS = 64
PHYSICAL_CORES = 8   # core váº­t lÃ½


# =====================
# ONNX Runtime options
# =====================
sess_options = ort.SessionOptions()
sess_options.intra_op_num_threads = PHYSICAL_CORES
sess_options.inter_op_num_threads = 1
sess_options.execution_mode = ort.ExecutionMode.ORT_SEQUENTIAL
sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
sess_options.enable_mem_pattern = False
sess_options.enable_cpu_mem_arena = True


# =====================
# Load ONNX model
# =====================
model = ORTModelForSpeechSeq2Seq.from_pretrained(
    ONNX_DIR,
    provider="CPUExecutionProvider",
    session_options=sess_options
)
processor = WhisperProcessor.from_pretrained(ONNX_DIR)


# =====================
# Read wav.scp (PATH ONLY)
# =====================
wav_paths = []
with open(WAV_SCP, "r", encoding="utf-8") as f:
    for line in f:
        p = line.strip()
        if p:
            wav_paths.append(p)

assert len(wav_paths) > 0, "wav.scp rá»—ng"


# =====================
# Warm-up (KHÃ”NG Ä‘o)
# =====================
warm_audio, _ = librosa.load(wav_paths[0], sr=16000)
warm_inputs = processor(
    warm_audio,
    sampling_rate=16000,
    return_tensors="pt"
)

with torch.no_grad():
    _ = model.generate(
        warm_inputs["input_features"],
        max_new_tokens=16,
        num_beams=1,
        do_sample=False
    )


# =====================
# REAL BENCHMARK
# =====================
total_audio_sec = 0.0
total_infer_time = 0.0

results = []

for wav_path in wav_paths:
    audio, _ = librosa.load(wav_path, sr=16000)
    duration = len(audio) / 16000
    total_audio_sec += duration

    inputs = processor(
        audio,
        sampling_rate=16000,
        return_tensors="pt"
    )

    start = time.time()
    with torch.no_grad():
        generated_ids = model.generate(
            inputs["input_features"],
            max_new_tokens=MAX_NEW_TOKENS,
            num_beams=1,          # â— báº¯t buá»™c
            do_sample=False,
            temperature=0.0,
            repetition_penalty=1.0,
            no_repeat_ngram_size=0,
            language=LANGUAGE,
            task=TASK
        )
    infer_time = time.time() - start
    total_infer_time += infer_time

    text = processor.batch_decode(
        generated_ids,
        skip_special_tokens=True
    )[0]

    utt_id = os.path.basename(wav_path)
    results.append((utt_id, duration, infer_time, text))


# =====================
# REPORT
# =====================
rtf = total_infer_time / total_audio_sec

print("=" * 60)
print(f"TOTAL FILES       : {len(results)}")
print(f"TOTAL AUDIO (sec) : {total_audio_sec:.2f}")
print(f"TOTAL INFER (sec) : {total_infer_time:.2f}")
print(f"RTF (CPU)         : {rtf:.3f}")
print("=" * 60)

for utt, dur, inf, _ in results:
    print(f"{utt:20s} {dur:6.2f}s â†’ {inf:6.2f}s")




import os
import time
import librosa
import torch
import onnxruntime as ort

from transformers import WhisperProcessor
from optimum.onnxruntime import ORTModelForSpeechSeq2Seq


# =========================
# CONFIG â€“ Tá»I Æ¯U
# =========================
ONNX_DIR = "./whisper_small_onnx"
AUDIO_PATH = "test.wav"

LANGUAGE = "vi"
TASK = "transcribe"

MAX_NEW_TOKENS = 64        # Cá»°C Ká»² QUAN TRá»ŒNG
CHUNK_SECONDS = 5          # chunk ngáº¯n = nhanh hÆ¡n
SAMPLE_RATE = 16000

PHYSICAL_CORES = 8         # â— dÃ¹ng core váº­t lÃ½, KHÃ”NG dÃ¹ng thread
WARMUP = True


# =========================
# ONNX Runtime tá»‘i Æ°u CPU
# =========================
sess_options = ort.SessionOptions()
sess_options.intra_op_num_threads = PHYSICAL_CORES
sess_options.inter_op_num_threads = 1
sess_options.execution_mode = ort.ExecutionMode.ORT_SEQUENTIAL
sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
sess_options.enable_mem_pattern = False
sess_options.enable_cpu_mem_arena = True


# =========================
# Load ONNX model
# =========================
model = ORTModelForSpeechSeq2Seq.from_pretrained(
    ONNX_DIR,
    provider="CPUExecutionProvider",
    session_options=sess_options
)

processor = WhisperProcessor.from_pretrained(ONNX_DIR)


# =========================
# Load audio
# =========================
audio, _ = librosa.load(AUDIO_PATH, sr=SAMPLE_RATE)
audio_len = len(audio) / SAMPLE_RATE


# =========================
# Chunking (ÄÃ’N Máº NH NHáº¤T)
# =========================
def chunk_audio(audio, chunk_sec):
    chunk_size = int(chunk_sec * SAMPLE_RATE)
    return [audio[i:i + chunk_size] for i in range(0, len(audio), chunk_size)]


chunks = chunk_audio(audio, CHUNK_SECONDS)


# =========================
# Warm-up (ráº¥t quan trá»ng)
# =========================
if WARMUP:
    dummy = chunks[0]
    inputs = processor(dummy, sampling_rate=SAMPLE_RATE, return_tensors="pt")
    with torch.no_grad():
        _ = model.generate(
            inputs["input_features"],
            max_new_tokens=16,
            num_beams=1,
            do_sample=False
        )


# =========================
# Inference
# =========================
start = time.time()
texts = []

for chunk in chunks:
    inputs = processor(
        chunk,
        sampling_rate=SAMPLE_RATE,
        return_tensors="pt"
    )

    with torch.no_grad():
        generated_ids = model.generate(
            inputs["input_features"],
            max_new_tokens=MAX_NEW_TOKENS,
            num_beams=1,          # â— báº¯t buá»™c
            do_sample=False,
            temperature=0.0,
            no_repeat_ngram_size=0,
            repetition_penalty=1.0,
            language=LANGUAGE,
            task=TASK
        )

    text = processor.batch_decode(
        generated_ids,
        skip_special_tokens=True
    )[0]

    texts.append(text.strip())

end = time.time()


# =========================
# Output
# =========================
final_text = " ".join(texts)
infer_time = end - start
rtf = infer_time / audio_len

print("=" * 60)
print("TRANSCRIPTION:")
print(final_text)
print("-" * 60)
print(f"Audio length : {audio_len:.2f}s")
print(f"Infer time   : {infer_time:.2f}s")
print(f"RTF (CPU)    : {rtf:.3f}")
print("=" * 60)




import time
import librosa
import torch
import onnxruntime as ort

from transformers import WhisperProcessor
from optimum.onnxruntime import ORTModelForSpeechSeq2Seq


# =============================
# CONFIG
# =============================
MODEL_NAME = "openai/whisper-small"
AUDIO_PATH = "test.wav"
LANGUAGE = "vi"
MAX_NEW_TOKENS = 128
CPU_THREADS = 8   # chá»‰nh theo sá»‘ core mÃ¡y báº¡n


# =============================
# ONNX Runtime CPU optimization
# =============================
sess_options = ort.SessionOptions()
sess_options.intra_op_num_threads = CPU_THREADS
sess_options.inter_op_num_threads = 1
sess_options.execution_mode = ort.ExecutionMode.ORT_SEQUENTIAL
sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL


# =============================
# Load model (export ONNX náº¿u chÆ°a cÃ³)
# =============================
model = ORTModelForSpeechSeq2Seq.from_pretrained(
    MODEL_NAME,
    export=True,                      # chá»‰ export ONNX láº§n Ä‘áº§u
    provider="CPUExecutionProvider",
    session_options=sess_options
)

processor = WhisperProcessor.from_pretrained(MODEL_NAME)


# =============================
# Load & preprocess audio
# =============================
audio, sr = librosa.load(AUDIO_PATH, sr=16000)

# giá»›i háº¡n 30s cho á»•n Ä‘á»‹nh
audio = audio[:30 * 16000]

inputs = processor(
    audio,
    sampling_rate=16000,
    return_tensors="pt"
)


# =============================
# Warm-up (ráº¥t quan trá»ng cho benchmark)
# =============================
with torch.no_grad():
    _ = model.generate(
        inputs["input_features"],
        max_new_tokens=32
    )


# =============================
# Inference
# =============================
start = time.time()

with torch.no_grad():
    generated_ids = model.generate(
        inputs["input_features"],
        max_new_tokens=MAX_NEW_TOKENS,
        language=LANGUAGE,
        task="transcribe",
        do_sample=False
    )

end = time.time()


# =============================
# Decode
# =============================
text = processor.batch_decode(
    generated_ids,
    skip_special_tokens=True
)[0]


# =============================
# Output
# =============================
audio_duration = len(audio) / 16000
infer_time = end - start
rtf = infer_time / audio_duration

print("=" * 50)
print("TRANSCRIPTION:")
print(text)
print("-" * 50)
print(f"Audio length  : {audio_duration:.2f}s")
print(f"Infer time    : {infer_time:.2f}s")
print(f"RTF (CPU)     : {rtf:.3f}")
print("=" * 50)





import pyarrow.parquet as pq

# Ä‘á»c file Parquet
parquet_file = "dataset.parquet"
table = pq.ParquetFile(parquet_file)

# láº·p theo batch (chunk)
batch_size = 500_000
for batch in table.iter_batches(batch_size=batch_size):
    df_chunk = batch.to_pandas()  # chuyá»ƒn batch thÃ nh pandas DataFrame
    # xá»­ lÃ½ df_chunk á»Ÿ Ä‘Ã¢y
    print(df_chunk.shape)




for i, chunk in enumerate(pd.read_csv(csv_file, chunksize=chunksize, dtype={'label':'category'})):
    chunk.to_parquet(f"chunk_{i}.parquet")

# Äá»c láº¡i táº¥t cáº£ chunk báº±ng pandas.concat
import glob
all_chunks = [pd.read_parquet(f) for f in glob.glob("chunk_*.parquet")]
df = pd.concat(all_chunks, ignore_index=True)





import os
import pandas as pd

folder_path = "path/to/csv_folder"
csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]

combined_df = pd.DataFrame()

for i, file in enumerate(csv_files):
    file_path = os.path.join(folder_path, file)
    # File Ä‘áº§u tiÃªn giá»¯ header, cÃ¡c file sau bá» header
    df = pd.read_csv(file_path, header=0 if i==0 else None)
    
    # Náº¿u bá» header, gÃ¡n láº¡i tÃªn cá»™t tá»« file Ä‘áº§u tiÃªn
    if i > 0:
        df.columns = combined_df.columns

    combined_df = pd.concat([combined_df, df], ignore_index=True)

# Ghi ra CSV vá»›i index Ä‘Æ°á»£c reset tá»« 0
combined_df.to_csv("combined.csv", index=False)

print("ÄÃ£ gá»™p xong, chá»‰ giá»¯ header file Ä‘áº§u tiÃªn, index Ä‘Ã£ format láº¡i")






from phowhisper import PhoWhisper

model = PhoWhisper("small")

result = model.transcribe(
    "audio.wav",
    language="vi",
    beam_size=5,
    best_of=5,
    split_on_word=False,
    max_context_len=4096,
    chunk_length=30,      # QUAN TRá»ŒNG
    stride_length=5,      # QUAN TRá»ŒNG
    suppress_blank=True
)

for seg in result["segments"]:
    print(seg["start"], seg["end"], seg["text"])




def merge_segments(segments, gap_threshold=0.5):
    """
    segments: list dict {"speaker": int, "start": float, "end": float}
    gap_threshold: thá»i gian cho phÃ©p Ä‘á»ƒ merge (giÃ¢y)
    """
    if not segments:
        return []

    segments = sorted(segments, key=lambda x: x["start"])

    merged = [segments[0]]

    for seg in segments[1:]:
        last = merged[-1]

        # cÃ¹ng speaker
        if seg["speaker"] == last["speaker"]:
            # khoáº£ng cÃ¡ch nhá» -> merge
            if seg["start"] - last["end"] <= gap_threshold:
                last["end"] = seg["end"]
                continue

        # cÃ²n láº¡i: segment má»›i
        merged.append(seg)

    return merged





import torch
import torchaudio
import numpy as np
from sklearn.mixture import GaussianMixture
from sklearn_extra.cluster import KMedoids
from speechbrain.pretrained import EncoderClassifier
import torch.hub

# Load ECAPA (speaker embedding)
embedder = EncoderClassifier.from_hparams(
    source="speechbrain/spkrec-ecapa-voxceleb",
    run_opts={"device": "cpu"}
)

# Load VAD
vad_model, utils = torch.hub.load("snakers4/silero-vad", "silero_vad")
(get_speech_timestamps, save_audio, read_audio, VAD_iterator, collect_chunks) = utils

# Load audio
wav, sr = torchaudio.load("audio.wav")
wav = wav.squeeze()
if sr != 16000:
    wav = torchaudio.functional.resample(wav, sr, 16000)
    sr = 16000

# VAD
timestamps = get_speech_timestamps(wav, vad_model, sampling_rate=sr)

# Embedding
embs = []
for t in timestamps:
    seg = wav[t['start']:t['end']]
    emb = embedder.encode_batch(seg.unsqueeze(0)).squeeze().numpy()
    embs.append(emb)

embs = np.vstack(embs)

# ---- AUTO SPEAKER COUNT (BIC) ----
def auto_speaker_count(embeddings, max_speakers=8):
    bics = []
    for k in range(1, max_speakers+1):
        gmm = GaussianMixture(n_components=k, covariance_type='diag')
        gmm.fit(embeddings)
        bics.append(gmm.bic(embeddings))
    return np.argmin(bics) + 1

num_speakers = auto_speaker_count(embs)
print("Detected speakers:", num_speakers)

# ---- CLUSTERING ----
gmm = GaussianMixture(n_components=num_speakers).fit(embs)
labels = gmm.predict(embs)

# Print
for i, t in enumerate(timestamps):
    start = t['start']/sr
    end = t['end']/sr
    print(f"{start:.2f} -> {end:.2f} | SPK_{labels[i]}")





import torch
import torchaudio
import numpy as np
from sklearn.mixture import BayesianGaussianMixture
from speechbrain.pretrained import EncoderClassifier

# === Load ECAPA (speaker embedding) ===
embedder = EncoderClassifier.from_hparams(
    source="speechbrain/spkrec-ecapa-voxceleb",
    run_opts={"device": "cpu"}
)

# === Load Silero VAD ===
vad_model, utils = torch.hub.load("snakers4/silero-vad", "silero_vad")
(get_speech_timestamps, save_audio, read_audio, VAD_iterator, collect_chunks) = utils

# === Load audio ===
wav, sr = torchaudio.load("audio.wav")
wav = wav.squeeze()
if sr != 16000:
    wav = torchaudio.functional.resample(wav, sr, 16000)
    sr = 16000

# === VAD: láº¥y cÃ¡c Ä‘oáº¡n cÃ³ tiáº¿ng nÃ³i ===
timestamps = get_speech_timestamps(wav, vad_model, sampling_rate=sr)

embs = []
for t in timestamps:
    seg = wav[t['start']:t['end']]
    emb = embedder.encode_batch(seg.unsqueeze(0)).squeeze().numpy()
    embs.append(emb)

embs = np.vstack(embs)

# === DPGMM: model tá»± tÃ¬m sá»‘ ngÆ°á»i ===
dpgmm = BayesianGaussianMixture(
    n_components=30,
    covariance_type="full",
    weight_concentration_prior_type="dirichlet_process",
    weight_concentration_prior=1e-3
)

dpgmm.fit(embs)
labels = dpgmm.predict(embs)

speakers = np.unique(labels)
print("Detected speakers:", len(speakers))

# === Print result ===
for i, t in enumerate(timestamps):
    print(f"{t['start']/sr:.2f} -> {t['end']/sr:.2f} | SPK_{labels[i]}")




import torch
import torchaudio
import numpy as np
from sklearn.cluster import KMeans
from speechbrain.pretrained import EncoderClassifier
import torch.hub

# Load ECAPA (speaker embedding)
embedder = EncoderClassifier.from_hparams(
    source="speechbrain/spkrec-ecapa-voxceleb",
    run_opts={"device": "cpu"}
)

# Load VAD
vad_model, utils = torch.hub.load("snakers4/silero-vad", "silero_vad")
(get_speech_timestamps, save_audio, read_audio,
 VAD_iterator, collect_chunks) = utils

# Load audio
wav, sr = torchaudio.load("audio.wav")
wav = wav.squeeze()
if sr != 16000:
    wav = torchaudio.functional.resample(wav, sr, 16000)
    sr = 16000

# 1) VAD tÃ¡ch thÃ nh cÃ¡c Ä‘oáº¡n
ts = get_speech_timestamps(wav, vad_model, sampling_rate=sr)

segments = [wav[t['start']:t['end']] for t in ts]

# 2) Láº¥y embedding ngÆ°á»i nÃ³i
embs = []
for seg in segments:
    emb = embedder.encode_batch(seg.unsqueeze(0)).squeeze().numpy()
    embs.append(emb)

embs = np.vstack(embs)

# 3) KMeans phÃ¢n cá»¥m (2 ngÆ°á»i nÃ³i)
kmeans = KMeans(n_clusters=2)
labels = kmeans.fit_predict(embs)

# 4) Káº¿t quáº£ diarization
for i, t in enumerate(ts):
    print(f"{t['start']/sr:.2f} -> {t['end']/sr:.2f} | SPK_{labels[i]}")





# app.py
"""
Single-file FastAPI app:
 - Upload audio -> convert -> faster-whisper ASR -> pyannote diarization -> merge -> JSON response
 - Requires: ffmpeg installed system-wide, HuggingFace token exported for pyannote
 - Usage: uvicorn app:app --host 0.0.0.0 --port 8000
"""

import os
import uuid
import shutil
import tempfile
import math
from typing import List
from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.responses import JSONResponse
from pydub import AudioSegment
import soundfile as sf
import numpy as np

# === ASR: faster-whisper ===
try:
    from faster_whisper import WhisperModel
except Exception as e:
    raise ImportError("Please install faster-whisper. pip install faster-whisper") from e

# === Diarization: pyannote ===
try:
    from pyannote.audio import Pipeline
except Exception as e:
    raise ImportError("Please install pyannote.audio and set HUGGINGFACE token. pip install pyannote.audio") from e

# === CONFIG ===
# Model choice: "small", "medium", "large-v2", etc.
MODEL_NAME = os.environ.get("WHISPER_MODEL", "medium")
# Device: "cuda" or "cpu"
DEVICE = os.environ.get("DEVICE", "cuda")
# compute type for faster-whisper (float16 / int8 / float32)
COMPUTE_TYPE = os.environ.get("COMPUTE_TYPE", "float16")

# Pyannote pipeline id (change if needed)
PYANNOTE_PIPELINE_ID = os.environ.get("PYANNOTE_PIPELINE_ID", "pyannote/speaker-diarization")

# Safety limits
MAX_AUDIO_SECONDS = int(os.environ.get("MAX_AUDIO_SECONDS", 60 * 60))  # default 1 hour

# Create simple data dir
BASE_DIR = os.path.abspath(os.getcwd())
DATA_DIR = os.path.join(BASE_DIR, "data")
os.makedirs(DATA_DIR, exist_ok=True)

app = FastAPI(title="ASR + Diarization (single-file)")

# === Initialize models once ===
print(f"[startup] Loading WhisperModel {MODEL_NAME} on {DEVICE} (compute_type={COMPUTE_TYPE}) ...")
whisper_model = WhisperModel(MODEL_NAME, device=DEVICE, compute_type=COMPUTE_TYPE)
print("[startup] WhisperModel loaded.")

print(f"[startup] Loading pyannote pipeline '{PYANNOTE_PIPELINE_ID}' ...")
# Pyannote needs HF token via env var: HUGGINGFACE_TOKEN or PYANNOTE_AUDIO_AUTH_TOKEN
hf_token = os.environ.get("HUGGINGFACE_TOKEN") or os.environ.get("PYANNOTE_AUDIO_AUTH_TOKEN")
if not hf_token:
    print("WARNING: No Hugging Face token found in HUGGINGFACE_TOKEN / PYANNOTE_AUDIO_AUTH_TOKEN. Pyannote.from_pretrained may fail.")
pipeline = Pipeline.from_pretrained(PYANNOTE_PIPELINE_ID, use_auth_token=hf_token)
print("[startup] Pyannote pipeline loaded.")


# === Utility: audio conversion ===
def ensure_wav_16k_mono(src_path: str, dst_path: str) -> str:
    """
    Convert any audio file to 16kHz WAV mono using pydub (ffmpeg required).
    Returns dst_path.
    """
    audio = AudioSegment.from_file(src_path)
    audio = audio.set_frame_rate(16000).set_channels(1)
    audio.export(dst_path, format="wav")
    return dst_path

def get_audio_duration_seconds(path: str) -> float:
    audio = AudioSegment.from_file(path)
    return len(audio) / 1000.0

# === ASR wrapper (segment-level) ===
def transcribe_segments(audio_path: str, language: str = "vi"):
    """
    Use faster-whisper to transcribe.
    Returns list of dicts: [{'start': float,'end': float,'text': str}, ...]
    """
    segments_out = []
    # faster-whisper returns generator of Segment objects plus info
    segments, info = whisper_model.transcribe(audio_path, language=language, beam_size=5)
    for seg in segments:
        segments_out.append({
            "start": float(seg.start),
            "end": float(seg.end),
            "text": seg.text.strip()
        })
    return segments_out

# === Diarization wrapper ===
def run_diarization(audio_path: str):
    """
    Run pyannote diarization pipeline.
    Returns list of {'start': float, 'end': float, 'speaker': str}
    """
    diar = pipeline(audio_path)
    segments = []
    # diar.itertracks yields (segment, track, label)
    for segment, _, label in diar.itertracks(yield_label=True):
        segments.append({
            "start": float(segment.start),
            "end": float(segment.end),
            "speaker": label
        })
    return segments

# === Merge logic ===
def assign_speaker_to_segment(asr_seg: dict, diar_segments: List[dict]) -> str:
    """
    Assign speaker label to an ASR segment using midpoint overlap.
    Fallback: nearest diarization segment by midpoint distance.
    """
    midpoint = (asr_seg["start"] + asr_seg["end"]) / 2.0
    # Direct cover
    for d in diar_segments:
        if d["start"] <= midpoint <= d["end"]:
            return d["speaker"]
    # fallback nearest by distance
    if diar_segments:
        nearest = min(diar_segments, key=lambda x: abs((x["start"] + x["end"]) / 2.0 - midpoint))
        return nearest["speaker"]
    return "SPEAKER_00"

def merge_asr_and_diar(asr_segments: List[dict], diar_segments: List[dict], join_gap: float = 1.0):
    """
    Merge ASR segments with speaker labels, then join adjacent segments by same speaker.
    join_gap: max gap in seconds to join adjacent segments from same speaker.
    Returns grouped list: [{'speaker','start','end','text'}...]
    """
    merged = []
    for s in asr_segments:
        spk = assign_speaker_to_segment(s, diar_segments)
        merged.append({
            "speaker": spk,
            "start": s["start"],
            "end": s["end"],
            "text": s["text"]
        })
    # Sort by start
    merged.sort(key=lambda x: x["start"])
    # Group continuous segments by speaker
    if not merged:
        return []
    grouped = []
    cur = merged[0].copy()
    for seg in merged[1:]:
        if seg["speaker"] == cur["speaker"] and seg["start"] - cur["end"] <= join_gap:
            # extend
            cur["end"] = seg["end"]
            cur["text"] = cur["text"] + " " + seg["text"]
        else:
            grouped.append(cur)
            cur = seg.copy()
    grouped.append(cur)
    return grouped

# === FastAPI endpoints ===
@app.post("/transcribe")
async def transcribe(file: UploadFile = File(...), language: str = "vi"):
    """
    Upload audio file -> return JSON with 'transcript' (speaker segments) and duration_seconds.
    """
    uid = uuid.uuid4().hex[:8]
    tmp_dir = tempfile.mkdtemp(prefix=f"asr_{uid}_", dir=DATA_DIR)
    orig_path = os.path.join(tmp_dir, f"orig_{file.filename}")
    try:
        # save uploaded file
        with open(orig_path, "wb") as f:
            shutil.copyfileobj(file.file, f)
    except Exception as e:
        shutil.rmtree(tmp_dir, ignore_errors=True)
        raise HTTPException(status_code=500, detail=f"Failed to save upload: {e}")

    wav_path = os.path.join(tmp_dir, f"{uid}.wav")
    try:
        ensure_wav_16k_mono(orig_path, wav_path)
    except Exception as e:
        shutil.rmtree(tmp_dir, ignore_errors=True)
        raise HTTPException(status_code=500, detail=f"Audio convert failed (ffmpeg required): {e}")

    # duration check
    duration = get_audio_duration_seconds(wav_path)
    if duration > MAX_AUDIO_SECONDS:
        shutil.rmtree(tmp_dir, ignore_errors=True)
        raise HTTPException(status_code=400, detail=f"Audio too long ({duration}s). Max allowed {MAX_AUDIO_SECONDS}s.")

    # 1) ASR
    try:
        asr_segments = transcribe_segments(wav_path, language=language)
    except Exception as e:
        shutil.rmtree(tmp_dir, ignore_errors=True)
        raise HTTPException(status_code=500, detail=f"ASR failed: {e}")

    # 2) Diarization
    try:
        diar_segments = run_diarization(wav_path)
    except Exception as e:
        # If diarization fails, we still can return ASR without speakers
        diar_segments = []
        print(f"[warn] Diarization failed: {e}")

    # 3) Merge
    transcript = merge_asr_and_diar(asr_segments, diar_segments)

    # clean temp (keep wav if you want; here we remove everything)
    shutil.rmtree(tmp_dir, ignore_errors=True)

    return JSONResponse({"transcript": transcript, "duration_seconds": duration})

@app.get("/health")
def health():
    return {"status": "ok", "model": MODEL_NAME, "device": DEVICE}

# === If run directly ===
if __name__ == "__main__":
    import uvicorn
    print("Starting uvicorn at http://0.0.0.0:8000 ...")
    uvicorn.run("app:app", host="0.0.0.0", port=8000, reload=True)




import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import HypergraphConv
from torch_geometric.data import Data
from torch.utils.data import DataLoader, TensorDataset

# -------------------------
# 1. Táº¡o dataset giáº£ láº­p lá»›n
# -------------------------
N = 50000   # sá»‘ node lá»›n
F_dim = 16  # feature dim
C = 10      # sá»‘ class

x = torch.randn(N, F_dim)
y = torch.randint(0, C, (N,))

# Táº¡o hyperedges giáº£ láº­p: má»—i hyperedge ná»‘i 5 node
num_hyperedges = 10000
hyperedges = torch.randint(0, N, (5, num_hyperedges))
hyperedge_index = torch.cat([hyperedges[0:2], hyperedges[2:4], hyperedges[4:5]], dim=0)  # COO Ä‘Æ¡n giáº£n

data = Data(x=x, y=y)
data.hyperedge_index = hyperedge_index

# -------------------------
# 2. HyperGCN model
# -------------------------
class HyperGCN(nn.Module):
    def __init__(self, in_dim, hid_dim, out_dim):
        super().__init__()
        self.conv1 = HypergraphConv(in_dim, hid_dim)
        self.conv2 = HypergraphConv(hid_dim, out_dim)
    
    def forward(self, x, hyperedge_index):
        x = F.relu(self.conv1(x, hyperedge_index))
        x = self.conv2(x, hyperedge_index)
        return F.normalize(x, dim=1)  # chuáº©n hÃ³a embeddings

model = HyperGCN(in_dim=F_dim, hid_dim=64, out_dim=32).cuda()

# -------------------------
# 3. SupCon loss memory-efficient
# -------------------------
def supcon_loss_memory_efficient(features, labels, batch_size=1024, temperature=0.07, num_negatives=128):
    device = features.device
    N = features.shape[0]
    loss_total = 0.0
    count = 0

    for start in range(0, N, batch_size):
        end = min(start + batch_size, N)
        anchor_features = features[start:end]
        anchor_labels = labels[start:end].to(device)

        # positive mask
        mask_pos = (anchor_labels.view(-1,1) == labels.to(device).view(1,-1)).float()
        # remove self-contrast
        mask_pos[:, start:end] -= torch.eye(end-start, device=device)

        # negative sampling
        mask_neg = (anchor_labels.view(-1,1) != labels.to(device).view(1,-1)).float()
        neg_idx = torch.randint(0, N, (end-start, num_negatives), device=device)
        mask_neg = mask_neg.gather(1, neg_idx)

        # compute logits
        pos_logits = torch.matmul(anchor_features, features.T) / temperature
        pos_logits = pos_logits * mask_pos
        neg_logits = torch.matmul(anchor_features, features.T) / temperature
        neg_logits = neg_logits * mask_neg

        # log-softmax
        logits = torch.cat([pos_logits, neg_logits], dim=1)
        log_prob = F.log_softmax(logits, dim=1)

        loss_batch = - (mask_pos * log_prob[:, :mask_pos.shape[1]]).sum(1) / mask_pos.sum(1).clamp(min=1)
        loss_total += loss_batch.sum().item()
        count += (end-start)
    return loss_total / count

# -------------------------
# 4. Training loop mini-batch
# -------------------------
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
x, y = x.cuda(), y.cuda()

for epoch in range(5):
    model.train()
    optimizer.zero_grad()
    embeddings = model(x, hyperedge_index.cuda())
    loss = supcon_loss_memory_efficient(embeddings, y, batch_size=2048, num_negatives=256)
    loss_tensor = torch.tensor(loss, requires_grad=True, device='cuda')
    loss_tensor.backward()
    optimizer.step()
    print(f"Epoch {epoch}, SupCon Loss: {loss:.4f}")





def supcon_loss_sampled(features, labels, temperature=0.07, num_negatives=64):
    device = features.device
    N, D = features.shape
    loss = 0
    for i in range(N):
        anchor = features[i]
        pos_mask = (labels == labels[i]).nonzero(as_tuple=True)[0]
        pos_mask = pos_mask[pos_mask != i]  # loáº¡i chÃ­nh nÃ³
        if len(pos_mask) == 0:
            continue
        pos_samples = features[pos_mask]
        # random negatives
        neg_mask = (labels != labels[i]).nonzero(as_tuple=True)[0]
        neg_idx = torch.randperm(len(neg_mask))[:num_negatives]
        neg_samples = features[neg_mask[neg_idx]]
        logits_pos = torch.matmul(anchor, pos_samples.T) / temperature
        logits_neg = torch.matmul(anchor, neg_samples.T) / temperature
        logits = torch.cat([logits_pos, logits_neg])
        labels_i = torch.zeros(len(logits))
        labels_i[:len(pos_samples)] = 1
        labels_i = labels_i.to(device)
        loss += F.binary_cross_entropy_with_logits(logits, labels_i)
    return loss / N




def info_sup_contrastive_loss(z1, z2, y, tau=0.2, sup_weight=1.0, batch_size=2048):
    device = z1.device
    N = z1.size(0)
    loss_all = []

    for start in range(0, N, batch_size):
        end = min(start + batch_size, N)
        z1_b = safe_normalize(z1[start:end])
        z2_b = safe_normalize(z2[start:end])
        y_b = y[start:end]

        # ----- InfoNCE -----
        logits_info = torch.matmul(z1_b, z2_b.T) / tau
        labels_info = torch.arange(end-start, device=device)
        loss_info = F.cross_entropy(logits_info, labels_info)

        # ----- SupCon -----
        z_b = torch.cat([z1_b, z2_b], dim=0)  # (2B, D)
        y_b2 = y_b.repeat(2)
        sim = torch.matmul(z_b, z_b.T) / tau

        mask = torch.eye(2*(end-start), dtype=torch.bool, device=device)
        sim.masked_fill_(mask, -1e9)

        pos_mask = (y_b2.unsqueeze(0) == y_b2.unsqueeze(1)) & (~mask)
        exp_sim = torch.exp(sim)
        numerator = (exp_sim * pos_mask).sum(dim=1)
        denominator = exp_sim.sum(dim=1)
        loss_sup = -torch.log((numerator + 1e-8) / (denominator + 1e-8))
        loss_sup = loss_sup.mean()

        # combine
        loss_all.append(loss_info + sup_weight * loss_sup)

    return torch.mean(torch.stack(loss_all))






import torch
import torch.nn.functional as F

def safe_normalize(z, dim=1, eps=1e-6):
    """
    Convert to float if needed, handle empty, NaN/Inf, then normalize.
    Returns normalized tensor with same device.
    """
    if not torch.is_tensor(z):
        raise TypeError("safe_normalize expects a torch.Tensor")

    # ensure float
    if not z.dtype.is_floating_point:
        z = z.float()

    # shape check: expect 2D (N, D)
    if z.ndim != 2:
        raise ValueError(f"safe_normalize expects 2D tensor (N, D). Got ndim={z.ndim}, shape={z.shape}")

    # replace NaN/Inf (if any)
    if torch.isnan(z).any() or torch.isinf(z).any():
        z = torch.nan_to_num(z, nan=0.0, posinf=1e6, neginf=-1e6)

    # if a row is all zeros, add tiny noise to avoid zero-norm
    row_norm = torch.linalg.vector_norm(z, dim=1)
    zero_rows = (row_norm == 0)
    if zero_rows.any():
        z[zero_rows] = z[zero_rows] + (torch.randn_like(z[zero_rows]) * eps)

    # finally normalize
    z = F.normalize(z, dim=dim)
    return z

# usage
z2 = safe_normalize(z2, dim=1)



def info_nce_loss_minibatch(z1, z2, batch_size=4096, tau=0.2):
    N = z1.size(0)
    loss_all = []

    for start in range(0, N, batch_size):
        end = min(start + batch_size, N)

        z1_b = F.normalize(z1[start:end], dim=1)
        z2_b = F.normalize(z2[start:end], dim=1)

        # negatives chá»‰ trong batch â†’ khÃ´ng OOM
        sim = torch.matmul(z1_b, z2_b.T) / tau

        labels = torch.arange(end - start, device=z1.device)

        loss = F.cross_entropy(sim, labels)
        loss_all.append(loss)

    return torch.mean(torch.stack(loss_all))




import torch
import torch.nn.functional as F

def prototype_contrastive_loss(z, y, tau=0.2, eps=1e-8):
    """
    z: [N, d]   node embeddings
    y: [N]      labels
    tau: temperature
    """

    # ---- 1. Normalize ----
    z = F.normalize(z, dim=1)

    # ---- 2. Compute prototypes by class ----
    classes = torch.unique(y)
    prototypes = []

    for c in classes:
        proto = z[y == c].mean(dim=0)
        prototypes.append(proto)

    prototypes = torch.stack(prototypes)                     # [C, d]
    prototypes = F.normalize(prototypes, dim=1).detach()     # <-- detach quan trá»ng

    # ---- 3. Cosine sim: node vs all prototypes ----
    sim = torch.matmul(z, prototypes.T) / tau                # [N, C]

    # ---- 4. Map labels â†’ prototype index ----
    mapping = {c.item(): i for i, c in enumerate(classes)}
    target = torch.tensor([mapping[label.item()] for label in y],
                          device=z.device)

    # ---- 5. Class-balanced loss (fix imbalance) ----
    counts = torch.tensor(
        [ (y == c).sum().item() for c in classes ],
        device=z.device, dtype=torch.float
    )
    weights = 1.0 / (counts + eps)                           # inverse freq
    weights = weights / weights.sum()                       # normalize

    # ---- 6. Cross entropy with class weights ----
    loss = F.cross_entropy(sim, target, weight=weights)

    return loss





import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import HypergraphConv
from sklearn.model_selection import train_test_split
from collections import Counter

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ---------- Load .pt ----------
data = torch.load("your_hypergraph.pt")
# expect: data.x [N, F], data.y [N], data.edge_index [2, E] (node, hyperedge)
x = data.x
y = data.y
edge_index = data.edge_index  # [2, E]

# ---------- train/test split (stratified) ----------
all_idx = torch.arange(x.size(0))
train_idx_np, test_idx_np = train_test_split(all_idx.numpy(), test_size=0.2, stratify=y.numpy(), random_state=42)
train_idx = torch.tensor(train_idx_np, dtype=torch.long)
test_idx = torch.tensor(test_idx_np, dtype=torch.long)

# ---------- helper: build subgraph with reindexing ----------
def build_subgraph(node_idx, edge_index):
    """
    node_idx: 1D tensor of original node indices to keep
    edge_index: [2, E] (node, hyperedge)
    returns:
      new_x_idx_map: dict old->new
      sub_edge_index: [2, E_sub] with node ids remapped 0..n-1
      keep_node_idx (tensor): original indices in same order as mapping
    """
    node_idx = node_idx.cpu()
    n_keep = node_idx.numel()
    old2new = -torch.ones(edge_index[0].max().item()+1, dtype=torch.long)
    old2new[node_idx] = torch.arange(n_keep, dtype=torch.long)
    # mask edges columns where node in train
    mask = torch.isin(edge_index[0], node_idx)
    ei = edge_index[:, mask]
    # group by hyperedge id: for each hyperedge collect nodes that are in node_idx
    he_ids = torch.unique(ei[1])
    new_cols = []
    for he in he_ids:
        nodes = ei[0][ei[1]==he]
        nodes_new = old2new[nodes]
        # drop hyperedge if less than 2 nodes after filter
        if nodes_new.numel() < 2:
            continue
        he_new_col = torch.stack([nodes_new, torch.full((nodes_new.size(0),), he, dtype=torch.long)], dim=0)
        new_cols.append(he_new_col)
    if len(new_cols)==0:
        return old2new, torch.empty((2,0), dtype=torch.long), node_idx
    sub_edge_index = torch.cat(new_cols, dim=1)
    return old2new, sub_edge_index, node_idx

old2new_train, edge_index_train, train_keep = build_subgraph(train_idx, edge_index)
# edge_index_train nodes are remapped 0..N_train-1 but hyperedge ids remain original ids (not contiguous) - that's ok

# build x_train, y_train in remapped order
x_train = x[train_keep].to(device)
y_train = y[train_keep].to(device)

x_test = x[test_idx].to(device)
y_test = y[test_idx].to(device)

edge_index_train = edge_index_train.to(device)

# ---------- identify rare classes in train set ----------
label_counts = Counter(y_train.tolist())
threshold = 10
rare_classes = [c for c, cnt in label_counts.items() if cnt < threshold]
rare_mask = torch.isin(y_train, torch.tensor(rare_classes, device=y_train.device))
rare_nodes_local_idx = rare_mask.nonzero(as_tuple=True)[0]  # indices in 0..N_train-1
# collect hyperedges that involve rare nodes (hyperedge ids are original he ids)
rare_hyperedges = torch.unique(edge_index_train[1][torch.isin(edge_index_train[0], rare_nodes_local_idx)]).to(device)

# ---------- augmentation helpers (operate on remapped train subgraph) ----------
def feature_jitter_subset(x_sub, nodes_local_idx, sigma=0.02):
    x_aug = x_sub.clone()
    if nodes_local_idx.numel() > 0:
        x_aug[nodes_local_idx] = x_aug[nodes_local_idx] + torch.randn_like(x_aug[nodes_local_idx]) * sigma
    return x_aug

def drop_nodes_in_hyperedge_sub(edge_index_sub, rare_hyperedges_ids, drop_rate=0.3):
    new_cols = []
    he_ids = torch.unique(edge_index_sub[1])
    for he in he_ids:
        cols = (edge_index_sub[1] == he)
        nodes = edge_index_sub[0, cols]
        if he in rare_hyperedges_ids:
            keep_mask = torch.rand(nodes.size(0), device=nodes.device) > drop_rate
            if keep_mask.sum() < 2:
                # ensure at least 2
                keep_idx = torch.arange(nodes.size(0), device=nodes.device)
                keep_mask[keep_idx[:2]] = True
            nodes = nodes[keep_mask]
        # skip if <2 nodes
        if nodes.numel() < 2:
            continue
        he_cols = torch.stack([nodes, torch.full((nodes.size(0),), he, dtype=torch.long, device=nodes.device)], dim=0)
        new_cols.append(he_cols)
    if len(new_cols)==0:
        return torch.empty((2,0), dtype=torch.long, device=edge_index_sub.device)
    return torch.cat(new_cols, dim=1)

# ---------- Encoder ----------
class HyperGCL_Encoder(nn.Module):
    def __init__(self, in_dim, hid_dim, out_dim):
        super().__init__()
        self.conv1 = HypergraphConv(in_dim, hid_dim)
        self.conv2 = HypergraphConv(hid_dim, out_dim)
    def forward(self, x_local, edge_index_local):
        x = F.relu(self.conv1(x_local, edge_index_local))
        x = self.conv2(x, edge_index_local)
        return x

# ---------- prototype contrastive loss (robust) ----------
def prototype_contrastive_loss(z, y, tau=0.5, eps=1e-6):
    # z: [N, d] normalized inside
    # y: [N] local labels (may contain subset of classes)
    device = z.device
    classes = torch.unique(y)
    prototypes = []
    valid_classes = []
    for c in classes:
        mask = (y == c)
        if mask.sum() == 0:
            continue
        prot = z[mask].mean(dim=0)
        prototypes.append(prot)
        valid_classes.append(c)
    if len(prototypes) == 0:
        return torch.tensor(0.0, device=device, requires_grad=True)
    prototypes = torch.stack(prototypes, dim=0)
    # normalize
    z_n = F.normalize(z, dim=1)
    p_n = F.normalize(prototypes, dim=1)
    sim = torch.matmul(z_n, p_n.T) / tau
    # map y -> index in valid_classes
    c2i = {int(c.item()): idx for idx, c in enumerate(valid_classes)}
    target = torch.tensor([c2i[int(v.item())] for v in y], device=device)
    loss = F.cross_entropy(sim, target)
    return loss

# ---------- training ----------
in_dim = x_train.size(1)
hid_dim = 64
out_dim = 64
encoder = HyperGCL_Encoder(in_dim, hid_dim, out_dim).to(device)
opt = torch.optim.Adam(encoder.parameters(), lr=1e-3, weight_decay=1e-5)

epochs = 100
for epoch in range(epochs):
    encoder.train()
    # create two augmented views only on train subgraph
    x_v1 = feature_jitter_subset(x_train, rare_nodes_local_idx, sigma=0.02)
    e_v1 = drop_nodes_in_hyperedge_sub(edge_index_train, rare_hyperedges, drop_rate=0.3)
    x_v2 = feature_jitter_subset(x_train, rare_nodes_local_idx, sigma=0.02)
    e_v2 = drop_nodes_in_hyperedge_sub(edge_index_train, rare_hyperedges, drop_rate=0.3)

    # If augmentation removed all edges, fallback to original
    if e_v1.size(1) == 0:
        e_v1 = edge_index_train
    if e_v2.size(1) == 0:
        e_v2 = edge_index_train

    z1 = encoder(x_v1, e_v1)
    z2 = encoder(x_v2, e_v2)

    # combine
    z = torch.cat([z1, z2], dim=0)
    y_comb = torch.cat([y_train, y_train], dim=0)

    # normalize embeddings before proto loss
    z = F.normalize(z, dim=1)
    loss = prototype_contrastive_loss(z, y_comb, tau=0.5)

    opt.zero_grad()
    loss.backward()
    opt.step()

    if epoch % 10 == 0:
        print(f"Epoch {epoch} loss {loss.item():.4f}")

# ---------- Linear probe evaluation (train a simple classifier on frozen encoder) ----------
encoder.eval()
with torch.no_grad():
    # encode full train graph using original train edge_index (no augmentation)
    z_train_full = encoder(x_train, edge_index_train)
    z_test_full = encoder(x_test, edge_index_train)  # WARNING: if test hyperedges refer to nodes outside train, prefer building full graph encoder

# train small linear classifier on top of z_train_full
clf = nn.Linear(z_train_full.size(1), int(y.max().item())+1).to(device)
clf_opt = torch.optim.Adam(clf.parameters(), lr=1e-3, weight_decay=1e-5)
for it in range(200):
    clf.train()
    logits = clf(z_train_full)
    loss_clf = F.cross_entropy(logits, y_train)
    clf_opt.zero_grad()
    loss_clf.backward()
    clf_opt.step()
# evaluate
clf.eval()
with torch.no_grad():
    logits_test = clf(z_test_full)
    pred = logits_test.argmax(dim=1)
    acc = (pred == y_test).float().mean().item()
print("Linear probe test accuracy:", acc)





import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import HypergraphConv

# --------------------------
# 1ï¸âƒ£ Load data
# --------------------------
data = torch.load("your_hypergraph.pt")
# data.edge_index: [2, num_edges] nodeâ†”hyperedge
# data.x: node features [num_nodes, num_features]
# data.y: node labels [num_nodes]

# Identify rare classes
threshold = 10
from collections import Counter
label_counts = Counter(data.y.tolist())
rare_classes = [c for c, cnt in label_counts.items() if cnt < threshold]
rare_nodes_idx = torch.tensor([i for i, label in enumerate(data.y) if label in rare_classes])
rare_hyperedges_idx = torch.unique(data.edge_index[1][torch.isin(data.edge_index[0], rare_nodes_idx)])

# --------------------------
# 2ï¸âƒ£ Augmentation functions
# --------------------------
def feature_jitter(x, nodes_idx, sigma=0.02):
    x_aug = x.clone()
    x_aug[nodes_idx] += torch.randn_like(x_aug[nodes_idx]) * sigma
    return x_aug

def drop_nodes_in_hyperedge(edge_index, rare_hyperedges_idx, drop_rate=0.3):
    new_edge_list = []
    for he in torch.unique(edge_index[1]):
        nodes_in_he = edge_index[0, edge_index[1] == he]
        if he in rare_hyperedges_idx:
            keep_mask = torch.rand(len(nodes_in_he)) > drop_rate
            if keep_mask.sum() < 2:
                keep_mask[:2] = True
            nodes_in_he = nodes_in_he[keep_mask]
        he_indices = torch.full((len(nodes_in_he),), he, dtype=torch.long)
        new_edge_list.append(torch.stack([nodes_in_he, he_indices], dim=0))
    return torch.cat(new_edge_list, dim=1)

# --------------------------
# 3ï¸âƒ£ Hypergraph Encoder
# --------------------------
class HyperGCL_Encoder(nn.Module):
    def __init__(self, in_dim, hid_dim, out_dim):
        super().__init__()
        self.conv1 = HypergraphConv(in_dim, hid_dim)
        self.conv2 = HypergraphConv(hid_dim, out_dim)
    def forward(self, x, edge_index):
        x = F.relu(self.conv1(x, edge_index))
        x = self.conv2(x, edge_index)
        return x

# --------------------------
# 4ï¸âƒ£ Prototype Contrastive Loss
# --------------------------
def prototype_contrastive_loss(z, y, tau=0.5):
    classes = torch.unique(y)
    prototypes = torch.stack([z[y==c].mean(dim=0) for c in classes])
    z = F.normalize(z, dim=1)
    prototypes = F.normalize(prototypes, dim=1)
    sim = torch.matmul(z, prototypes.T) / tau
    label_to_idx = {c.item(): idx for idx, c in enumerate(classes)}
    target = torch.tensor([label_to_idx[label.item()] for label in y], device=z.device)
    loss = F.cross_entropy(sim, target)
    return loss

# --------------------------
# 5ï¸âƒ£ Training step (example)
# --------------------------
in_dim = data.x.size(1)
hid_dim = 64
out_dim = 64
encoder = HyperGCL_Encoder(in_dim, hid_dim, out_dim)

optimizer = torch.optim.Adam(encoder.parameters(), lr=0.01)

for epoch in range(50):
    # Augment rare class
    x_view1 = feature_jitter(data.x, rare_nodes_idx, sigma=0.02)
    edge_view1 = drop_nodes_in_hyperedge(data.edge_index, rare_hyperedges_idx, drop_rate=0.3)
    
    x_view2 = feature_jitter(data.x, rare_nodes_idx, sigma=0.02)
    edge_view2 = drop_nodes_in_hyperedge(data.edge_index, rare_hyperedges_idx, drop_rate=0.3)
    
    # Encode views
    z1 = encoder(x_view1, edge_view1)
    z2 = encoder(x_view2, edge_view2)
    
    # Combine embeddings
    z = torch.cat([z1, z2], dim=0)
    y = torch.cat([data.y, data.y], dim=0)
    
    # Prototype contrastive loss
    loss = prototype_contrastive_loss(z, y, tau=0.5)
    
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    if epoch % 5 == 0:
        print(f"Epoch {epoch}, Loss: {loss.item():.4f}")





import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import HypergraphConv

# -----------------------
# 1. Encoder + Projection
# -----------------------
class HyperEncoder(nn.Module):
    def __init__(self, in_dim=64, hid_dim=128):
        super().__init__()
        self.conv1 = HypergraphConv(in_dim, hid_dim)
        self.conv2 = HypergraphConv(hid_dim, hid_dim)

    def forward(self, x, hyperedge):
        he_idx, he_id = hyperedge
        x = self.conv1(x, he_idx, he_id)
        x = F.relu(x)
        x = self.conv2(x, he_idx, he_id)
        return x

class Projector(nn.Module):
    def __init__(self, in_dim=128, out_dim=64):
        super().__init__()
        self.mlp = nn.Sequential(
            nn.Linear(in_dim, in_dim),
            nn.ReLU(),
            nn.Linear(in_dim, out_dim)
        )

    def forward(self, z):
        return self.mlp(z)

# -----------------------
# 2. Contrastive Loss (InfoNCE)
# -----------------------
def contrastive_loss(z1, z2, tau=0.2):
    z1 = F.normalize(z1, dim=-1)
    z2 = F.normalize(z2, dim=-1)

    sim = z1 @ z2.t() / tau
    labels = torch.arange(sim.size(0)).long().cuda()
    return F.cross_entropy(sim, labels)

# -----------------------
# 3. Hypergraph Augmentation
# -----------------------
def augment(hyperedge_index, drop_rate=0.1):
    M = hyperedge_index.size(1)
    keep = int(M * (1 - drop_rate))
    idx = torch.randperm(M)[:keep]
    return hyperedge_index[:, idx], idx

# -----------------------
# 4. Training Loop (Batch Hyperedges)
# -----------------------
def train_contrastive(x, hyperedge_index, num_hyperedge, hyperedge_ptr, epochs=3, batch_size=1000):
    device = 'cuda' if torch.cuda.is_available() else 'cpu'

    encoder = HyperEncoder().to(device)
    proj = Projector().to(device)
    opt = torch.optim.Adam(list(encoder.parameters()) + list(proj.parameters()), lr=1e-3)

    x = x.to(device)

    for epoch in range(epochs):
        for h in range(0, num_hyperedge, batch_size):
            s = hyperedge_ptr[h]
            e = hyperedge_ptr[min(h + batch_size, num_hyperedge)]

            he_idx = hyperedge_index[:, s:e]
            he_id  = torch.arange(he_idx.size(1))

            # View 1
            he1_idx, _ = augment(he_idx)
            he1_id = torch.arange(he1_idx.size(1))
            he1 = (he1_idx.to(device), he1_id.to(device))

            # View 2
            he2_idx, _ = augment(he_idx)
            he2_id = torch.arange(he2_idx.size(1))
            he2 = (he2_idx.to(device), he2_id.to(device))

            # Encode
            z1 = encoder(x, he1)
            z2 = encoder(x, he2)

            # Projection
            p1 = proj(z1)
            p2 = proj(z2)

            # Loss
            loss = contrastive_loss(p1, p2)

            opt.zero_grad()
            loss.backward()
            opt.step()

        print(f"Epoch {epoch}  loss={loss.item():.4f}")

# -----------------------
# 5. Example Usage
# -----------------------
if __name__ == "__main__":
    # giáº£ sá»­ data báº¡n cÃ³:
    # x: (1694050, 64)
    # hyperedge_index: (2, M)
    # num_hyperedge = 60000
    # hyperedge_ptr: list start index cá»§a tá»«ng hyperedge
    x = torch.randn(1694050, 64)
    num_hyperedge = 60000
    hyperedge_index = torch.randint(0, 1694050, (2, 300000))  # vÃ­ dá»¥
    hyperedge_ptr = torch.linspace(0, hyperedge_index.size(1), steps=num_hyperedge + 1).long()

    train_contrastive(x, hyperedge_index, num_hyperedge, hyperedge_ptr)





import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import HypergraphConv

########################################
# 1) HyperGCN Model
########################################

class HyperGCN(nn.Module):
    def __init__(self, in_dim, hid_dim, out_dim):
        super().__init__()
        self.conv1 = HypergraphConv(in_dim, hid_dim)
        self.conv2 = HypergraphConv(hid_dim, out_dim)

    def forward(self, x, hyperedge_index):
        h = F.relu(self.conv1(x, hyperedge_index))
        z = self.conv2(h, hyperedge_index)
        return z

########################################
# 2) Augmentation Module (HyperGCL)
########################################

class AugmentModule:
    """HyperGCL augmentations"""

    # ---------- node drop ----------
    def node_drop(self, data: Data, drop_prob: float):
        x = data.x.clone()
        mask = torch.rand(x.size(0)) > drop_prob
        x = x * mask.unsqueeze(1)
        return self._clone_data(data, x=x)

    # ---------- feature mask ----------
    def feature_mask(self, data: Data, mask_prob: float):
        x = data.x.clone()
        mask = torch.rand_like(x) > mask_prob
        x = x * mask
        return self._clone_data(data, x=x)

    # ---------- hyperedge drop ----------
    def hyperedge_drop(self, data: Data, drop_prob: float):
        he = data.hyperedge_index
        num_edges = he[1].max().item() + 1
        mask = torch.rand(num_edges) > drop_prob
        keep = mask[he[1]]
        new_he = he[:, keep]
        return self._clone_data(data, hyperedge_index=new_he)

    # ---------- hyperedge perturb ----------
    def hyperedge_perturbation(self, data: Data, add_ratio=0.05, remove_ratio=0.05):
        he = data.hyperedge_index.clone()
        N = data.x.size(0)
        E = he[1].max().item() + 1

        keep = torch.rand(he.size(1)) > remove_ratio
        new_he = he[:, keep]

        num_add = int(he.size(1) * add_ratio)
        if num_add > 0:
            add_nodes = torch.randint(0, N, (num_add,))
            add_edges = torch.randint(0, E, (num_add,))
            add_he = torch.stack([add_nodes, add_edges], dim=0)
            new_he = torch.cat([new_he, add_he], dim=1)

        return self._clone_data(data, hyperedge_index=new_he)

    # ---------- helpers ----------
    def _clone_data(self, data: Data, x=None, hyperedge_index=None):
        return Data(
            x = x if x is not None else data.x.clone(),
            y = data.y,
            hyperedge_index = hyperedge_index if hyperedge_index is not None else data.hyperedge_index,
            train_mask = data.train_mask,
            val_mask = data.val_mask,
            test_mask = data.test_mask,
        )

    # ---------- pipeline ----------
    def apply(self, data, pipeline):
        d = data
        for (name, params) in pipeline:
            if name == "node_drop":
                d = self.node_drop(d, **params)
            elif name == "feature_mask":
                d = self.feature_mask(d, **params)
            elif name == "hyperedge_drop":
                d = self.hyperedge_drop(d, **params)
            elif name == "hyperedge_perturbation":
                d = self.hyperedge_perturbation(d, **params)
        return d

    def make_two_views(self, data, aug1, aug2):
        return self.apply(data, aug1), self.apply(data, aug2)


########################################
# 3) NT-Xent Contrastive Loss (SimCLR)
########################################

def contrastive_loss(z1, z2, temperature=0.5):
    z1 = F.normalize(z1, dim=1)
    z2 = F.normalize(z2, dim=1)

    B = z1.size(0)
    sim = torch.mm(z1, z2.t()) / temperature
    labels = torch.arange(B).long().to(z1.device)
    return F.cross_entropy(sim, labels)


########################################
# 4) Full Training Function
########################################

def run_training(model, data, aug_module, epochs=200, lr=1e-3,
                 temperature=0.5, alpha=0.5, device='cpu'):

    model = model.to(device)
    data = data.to(device)
    opt = torch.optim.Adam(model.parameters(), lr=lr)

    # define augmentations
    aug1 = [
        ("node_drop", {"drop_prob": 0.2}),
        ("feature_mask", {"mask_prob": 0.2})
    ]
    aug2 = [
        ("hyperedge_drop", {"drop_prob": 0.2}),
        ("hyperedge_perturbation", {"add_ratio": 0.05, "remove_ratio": 0.05})
    ]

    for epoch in range(epochs):

        # ------- contrastive views -------
        v1, v2 = aug_module.make_two_views(data, aug1, aug2)
        v1 = v1.to(device)
        v2 = v2.to(device)

        model.train()
        z1 = model(v1.x, v1.hyperedge_index)
        z2 = model(v2.x, v2.hyperedge_index)

        loss_cl = contrastive_loss(z1, z2, temperature)

        z = model(data.x, data.hyperedge_index)
        loss_sup = F.cross_entropy(z[data.train_mask], data.y[data.train_mask])

        loss = alpha * loss_sup + (1 - alpha) * loss_cl

        opt.zero_grad()
        loss.backward()
        opt.step()

        if (epoch + 1) % 20 == 0:
            print(f"[Epoch {epoch+1}] Loss_sup={loss_sup.item():.4f}  Loss_cl={loss_cl.item():.4f}")

    # evaluate on test
    model.eval()
    out = model(data.x, data.hyperedge_index)
    pred = out.argmax(dim=1)
    test_acc = (pred[data.test_mask] == data.y[data.test_mask]).float().mean().item()

    print("=====================================")
    print(f" Test Accuracy: {test_acc:.4f}")
    print("=====================================")

    return model


########################################
# 5) Example synthetic data
########################################

if __name__ == "__main__":
    torch.manual_seed(42)

    x = torch.randn(100, 16)                     # node features
    y = torch.randint(0, 2, (100,))             # labels classification

    # random hypergraph H: N=100 nodes, E=20 hyperedges
    he_nodes = torch.randint(0, 100, (300,))
    he_edges = torch.randint(0, 20, (300,))
    hyperedge_index = torch.stack([he_nodes, he_edges], dim=0)

    data = Data(
        x=x,
        y=y,
        hyperedge_index=hyperedge_index,
        train_mask=torch.zeros(100, dtype=torch.bool).bernoulli(0.6),
        val_mask=torch.zeros(100, dtype=torch.bool).bernoulli(0.2),
        test_mask=torch.zeros(100, dtype=torch.bool).bernoulli(0.2)
    )

    model = HyperGCN(in_dim=16, hid_dim=32, out_dim=2)
    aug = AugmentModule()

    run_training(
        model=model,
        data=data,
        aug_module=aug,
        epochs=200,
        lr=1e-3,
        alpha=0.7
    )





import torch
import torch.nn as nn
from torch_geometric.nn import HypergraphConv

# Example HyperGCN model
class HyperGCN(nn.Module):
    def __init__(self, in_dim, hid_dim, out_dim):
        super().__init__()
        self.conv1 = HypergraphConv(in_dim, hid_dim)
        self.conv2 = HypergraphConv(hid_dim, out_dim)

    def forward(self, x, hyperedge_index):
        x = self.conv1(x, hyperedge_index)
        x = torch.relu(x)
        x = self.conv2(x, hyperedge_index)
        return x

# Example training loop

def train(model, data, epochs=200, lr=1e-3):
    model.train()
    opt = torch.optim.Adam(model.parameters(), lr=lr)
    loss_fn = nn.CrossEntropyLoss()

    x = data.x
    y = data.y
    hyperedge_index = data.hyperedge_index
    train_mask = data.train_mask

    for epoch in range(epochs):
        opt.zero_grad()
        out = model(x, hyperedge_index)
        loss = loss_fn(out[train_mask], y[train_mask])
        loss.backward()
        opt.step()

        if epoch % 20 == 0:
            print(f"Epoch {epoch} | Loss: {loss.item():.4f}")

# Usage:
# from torch_geometric.data import Data
# data = Data(x=node_features, y=labels, hyperedge_index=hyperedge_index, train_mask=train_mask)
# model = HyperGCN(in_dim=x.size(1), hid_dim=64, out_dim=num_classes)
# train(model, data)

# Added: test loop and full train+test pipeline

def test(model, data):
    model.eval()
    x, hyperedge_index = data.x, data.hyperedge_index
    out = model(x, hyperedge_index)
    pred = out.argmax(dim=1)
    acc = (pred[data.test_mask] == data.y[data.test_mask]).float().mean().item()
    print(f"Test Accuracy: {acc:.4f}")
    return acc

# Example full usage:
# data = Data(x=x, y=y, hyperedge_index=edge_index, train_mask=train_mask, val_mask=val_mask, test_mask=test_mask)
# model = HyperGCN(in_dim=x.size(1), hid_dim=64, out_dim=num_classes)
# train(model, data, epochs=200)
# test(model, data)

# ------------------ HyperGCL-style Contrastive + Supervised Training ------------------
import torch.nn.functional as F
from torch_geometric.nn import global_mean_pool


def nt_xent_loss(z1: Tensor, z2: Tensor, temperature: float = 0.5) -> Tensor:
    """Normalized temperature-scaled cross entropy loss for sets of graph embeddings.
    z1, z2: [batch, dim] (here batch = number of graphs/views; for node-level contrastive you would adapt)
    We compute contrastive loss pairing i in z1 with i in z2 (positive), others negative.
    """
    z1 = F.normalize(z1, p=2, dim=1)
    z2 = F.normalize(z2, p=2, dim=1)
    batch_size = z1.size(0)

    representations = torch.cat([z1, z2], dim=0)  # [2B, D]
    sim_matrix = torch.matmul(representations, representations.t())  # [2B,2B]
    sim_matrix = sim_matrix / temperature

    # mask out self-similarities
    mask = (~torch.eye(2 * batch_size, dtype=torch.bool, device=sim_matrix.device)).float()
    exp_sim = torch.exp(sim_matrix) * mask

    # positive pairs: i with i+B and i+B with i
    pos_sim = torch.exp(torch.sum(z1 * z2, dim=-1) / temperature)
    pos = torch.cat([pos_sim, pos_sim], dim=0)

    denom = exp_sim.sum(dim=1)
    loss = -torch.log(pos / denom)
    return loss.mean()


class HyperGCLTrainer:
    def __init__(self, model, aug_module, lr=1e-3, temperature=0.5, alpha=1.0, device='cpu'):
        """alpha: weight for supervised loss; contrastive weight = 1.0 by default"""
        self.model = model.to(device)
        self.aug = aug_module
        self.opt = torch.optim.Adam(self.model.parameters(), lr=lr)
        self.temperature = temperature
        self.alpha = alpha
        self.device = device

    def graph_repr(self, data: Data) -> Tensor:
        """Get graph-level representation by mean pooling node embeddings.
        Assumes single hypergraph per Data object.
        """
        self.model.eval()
        x = data.x.to(self.device)
        he = data.hyperedge_index.to(self.device)
        h = self.model(x, he)  # node embeddings
        # create a fake batch index all zeros (single graph) when pooling
        batch_index = torch.zeros(h.size(0), dtype=torch.long, device=h.device)
        g = global_mean_pool(h, batch_index)
        return h, g

    def train_one_epoch(self, data: Data, aug_params1, aug_params2):
        self.model.train()
        # create two augmented views
        v1, v2 = self.aug.make_two_views(data, aug_params1, aug_params2)
        # move to device
        v1.x = v1.x.to(self.device); v1.hyperedge_index = v1.hyperedge_index.to(self.device)
        v2.x = v2.x.to(self.device); v2.hyperedge_index = v2.hyperedge_index.to(self.device)

        # forward
        h1 = self.model(v1.x, v1.hyperedge_index)
        h2 = self.model(v2.x, v2.hyperedge_index)

        # graph-level embeddings (mean pool)
        batch1 = torch.zeros(h1.size(0), dtype=torch.long, device=h1.device)
        batch2 = torch.zeros(h2.size(0), dtype=torch.long, device=h2.device)
        g1 = global_mean_pool(h1, batch1)
        g2 = global_mean_pool(h2, batch2)

        # contrastive loss (between g1 and g2)
        contrastive_loss = nt_xent_loss(g1, g2, temperature=self.temperature)

        # supervised loss on original nodes (use original data labels)
        out = self.model(data.x.to(self.device), data.hyperedge_index.to(self.device))
        sup_loss = F.cross_entropy(out[data.train_mask.to(self.device)], data.y[data.train_mask].to(self.device))

        loss = contrastive_loss + self.alpha * sup_loss

        self.opt.zero_grad()
        loss.backward()
        self.opt.step()

        return loss.item(), contrastive_loss.item(), sup_loss.item()

    def validate(self, data: Data):
        self.model.eval()
        out = self.model(data.x.to(self.device), data.hyperedge_index.to(self.device))
        preds = out.argmax(dim=1)
        val_acc = (preds[data.val_mask.to(self.device)] == data.y[data.val_mask].to(self.device)).float().mean().item()
        return val_acc

    def test(self, data: Data):
        self.model.eval()
        out = self.model(data.x.to(self.device), data.hyperedge_index.to(self.device))
        preds = out.argmax(dim=1)
        test_acc = (preds[data.test_mask.to(self.device)] == data.y[data.test_mask].to(self.device)).float().mean().item()
        return test_acc


def run_training(model, data, aug_module, epochs=200, lr=1e-3, temperature=0.5, alpha=1.0, device='cpu'):
    trainer = HyperGCLTrainer(model, aug_module, lr=lr, temperature=temperature, alpha=alpha, device=device)

    # example augmentation pipelines (you can customize)
    aug1 = [('node_drop', {'drop_prob':0.2}), ('feature_mask', {'mask_prob':0.1})]
    aug2 = [('hyperedge_drop', {'drop_prob':0.3}), ('hyperedge_perturbation', {'add_ratio':0.05, 'remove_ratio':0.1})]

    best_val = 0.0
    best_model_state = None
    for epoch in range(1, epochs + 1):
        loss, cl_loss, sup_loss = trainer.train_one_epoch(data, aug1, aug2)
        if epoch % 10 == 0:
            val_acc = trainer.validate(data)
            print(f"Epoch {epoch:03d} | Loss {loss:.4f} | CL {cl_loss:.4f} | Sup {sup_loss:.4f} | ValAcc {val_acc:.4f}")
            if val_acc > best_val:
                best_val = val_acc
                best_model_state = {k: v.cpu() for k, v in trainer.model.state_dict().items()}

    # load best model and test
    if best_model_state is not None:
        trainer.model.load_state_dict(best_model_state)
    test_acc = trainer.test(data)
    print(f"Test Accuracy: {test_acc:.4f} | Best Val: {best_val:.4f}")
    return trainer.model

# End of HyperGCL-style training pipeline






import torch
from torch_geometric.data import HeteroData

# ==========================
# 1ï¸âƒ£ Táº¡o HeteroGraphData vá»›i multi-edges
# ==========================
data = HeteroData()

# Node features
data['node'].x = torch.tensor([[1.0, 0.5],  # node 0
                               [0.2, 1.5],  # node 1
                               [0.0, 0.1]]) # node 2

# Multi-edges: 0->1 (3 edges), 1->2 (2 edges)
edge_index = torch.tensor([
    [0, 0, 0, 1, 1],  # src
    [1, 1, 1, 2, 2]   # dst
])
edge_attr = torch.tensor([
    [1.0, 0.0],  # edge 0
    [0.5, 0.5],  # edge 1
    [0.0, 1.0],  # edge 2
    [1.0, 1.0],  # edge 3
    [0.0, 2.0]   # edge 4
])

data['node', 'to', 'node'].edge_index = edge_index
data['node', 'to', 'node'].edge_attr = edge_attr

# Reverse edges
data['node', 'rev_to', 'node'].edge_index = edge_index.flipud()
data['node', 'rev_to', 'node'].edge_attr = edge_attr.clone()

# ==========================
# 2ï¸âƒ£ Flatten edge mapping
# ==========================
def find_parallel_edges(edge_index):
    simplified_edge_mapping = {}
    simplified_edge_batch = []
    i = 0
    for edge in edge_index.T:
        tuple_edge = tuple(edge.tolist())
        if tuple_edge not in simplified_edge_mapping:
            simplified_edge_mapping[tuple_edge] = i
            i += 1
        simplified_edge_batch.append(simplified_edge_mapping[tuple_edge])
    return torch.LongTensor(simplified_edge_batch)

simp_edge_batch = find_parallel_edges(edge_index)

# GÃ¡n vÃ o graph
data['node', 'to', 'node'].simp_edge_batch = simp_edge_batch
data['node', 'rev_to', 'node'].simp_edge_batch = simp_edge_batch

print("simp_edge_batch:", simp_edge_batch)

# ==========================
# 3ï¸âƒ£ Flatten edge features (sum pooling)
# ==========================
def flatten_edge_features(edge_attr, simp_edge_batch):
    num_flattened_edges = simp_edge_batch.max().item() + 1
    flattened_edge_attr = torch.zeros((num_flattened_edges, edge_attr.size(1)))
    for i in range(num_flattened_edges):
        mask = simp_edge_batch == i
        flattened_edge_attr[i] = edge_attr[mask].sum(dim=0)
    return flattened_edge_attr

flat_attr_to = flatten_edge_features(data['node', 'to', 'node'].edge_attr,
                                     data['node', 'to', 'node'].simp_edge_batch)
flat_attr_rev = flatten_edge_features(data['node', 'rev_to', 'node'].edge_attr,
                                     data['node', 'rev_to', 'node'].simp_edge_batch)

print("Flattened edge features (to):\n", flat_attr_to)
print("Flattened edge features (rev):\n", flat_attr_rev)

# ==========================
# 4ï¸âƒ£ Káº¿t quáº£
# ==========================
# Edge 0->1: 3 multi-edges -> sum
# Edge 1->2: 2 multi-edges -> sum




import re

def analyze_text(text,
                 max_numbers=3,
                 max_english_words=5,
                 max_brackets=4):
    """
    Kiá»ƒm tra cÃ¢u xem cÃ³ quÃ¡ nhiá»u sá»‘, tá»« tiáº¿ng Anh, hoáº·c dáº¥u ngoáº·c.
    """
    # ---- Äáº¿m sá»‘ ----
    numbers = re.findall(r'\d+(\.\d+)?', text)
    num_count = len(numbers)

    # ---- Äáº¿m tá»« tiáº¿ng Anh ----
    english_words = re.findall(r'\b[a-zA-Z]+\b', text)
    eng_count = len(english_words)

    # ---- Äáº¿m dáº¥u ngoáº·c ----
    brackets = re.findall(r'[()\[\]{}]', text)
    br_count = len(brackets)

    # ---- Táº¡o thÃ´ng bÃ¡o ----
    messages = []

    if num_count > max_numbers:
        messages.append("QuÃ¡ nhiá»u sá»‘")

    if eng_count > max_english_words:
        messages.append("QuÃ¡ nhiá»u tá»« tiáº¿ng Anh")

    if br_count > max_brackets:
        messages.append("QuÃ¡ nhiá»u dáº¥u ngoáº·c")

    # Náº¿u khÃ´ng cÃ³ gÃ¬ báº¥t thÆ°á»ng
    if not messages:
        return "BÃ¬nh thÆ°á»ng"

    # GhÃ©p thÃ´ng bÃ¡o
    return ", ".join(messages)


# ---- VÃ­ dá»¥ ----
texts = [
    "GiÃ¡ lÃ  3 (láº§n), thÃªm 2 (option) vÃ  5 [more] items",
    "This is a test (a) (b) (c) (d) (e)",
    "HÃ´m nay cÃ³ 2 quáº£ tÃ¡o thÃ´i",
    "I bought 3 apples, 2 bananas and {one} more item"
]

for t in texts:
    print(f"'{t}' -> {analyze_text(t)}")





import re

def analyze_text(text, max_numbers=3, max_english_words=5):
    """
    Kiá»ƒm tra má»™t cÃ¢u xem cÃ³ quÃ¡ nhiá»u sá»‘ hoáº·c quÃ¡ nhiá»u tá»« tiáº¿ng Anh khÃ´ng.
    
    Args:
        text (str): CÃ¢u cáº§n kiá»ƒm tra
        max_numbers (int): NgÆ°á»¡ng tá»‘i Ä‘a sá»‘ lÆ°á»£ng sá»‘
        max_english_words (int): NgÆ°á»¡ng tá»‘i Ä‘a sá»‘ lÆ°á»£ng tá»« tiáº¿ng Anh
    
    Returns:
        str: ThÃ´ng bÃ¡o tÃ¬nh tráº¡ng cÃ¢u
    """
    # Äáº¿m sá»‘
    numbers = re.findall(r'\d+(\.\d+)?', text)
    num_count = len(numbers)
    
    # Äáº¿m tá»« tiáº¿ng Anh
    english_words = re.findall(r'\b[a-zA-Z]+\b', text)
    eng_count = len(english_words)
    
    # Kiá»ƒm tra ngÆ°á»¡ng
    if num_count > max_numbers and eng_count > max_english_words:
        return "QuÃ¡ nhiá»u sá»‘ vÃ  quÃ¡ nhiá»u tá»« tiáº¿ng Anh"
    elif num_count > max_numbers:
        return "QuÃ¡ nhiá»u sá»‘"
    elif eng_count > max_english_words:
        return "QuÃ¡ nhiá»u tá»« tiáº¿ng Anh"
    else:
        return "BÃ¬nh thÆ°á»ng"

# VÃ­ dá»¥ sá»­ dá»¥ng
texts = [
    "HÃ´m nay tÃ´i mua 3 apples, 2 oranges vÃ  5 bananas giÃ¡ 10 USD",
    "Chá»‰ cÃ³ 2 quáº£ tÃ¡o",
    "I love Python programming and AI"
]

for t in texts:
    print(f"'{t}' -> {analyze_text(t)}")





# train_whisper_v3.py
import os
import torch
import numpy as np
import soundfile as sf
from datasets import load_dataset, Audio
from transformers import (
    WhisperForConditionalGeneration,
    WhisperProcessor,
    Seq2SeqTrainingArguments,
    Seq2SeqTrainer
)
import evaluate
from dataclasses import dataclass
from typing import Any, Dict, List

# --- config ----------------------------------------------------------------------------
MODEL = "openai/whisper-large-v3"
TRAIN_CSV = "dataset_whisper/train.csv"
VALID_CSV = "dataset_whisper/valid.csv"
OUTPUT_DIR = "whisper_ft_v3"
BATCH_SIZE = 4
EPOCHS = 3
LEARNING_RATE = 1e-5
# ---------------------------------------------------------------------------------------

# processor + model
processor = WhisperProcessor.from_pretrained(MODEL)
model = WhisperForConditionalGeneration.from_pretrained(MODEL)

# set language + task
model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(
    language="vi", task="transcribe"
)
model.config.suppress_tokens = []

model.to("cuda")

# load datasets
data_files = {"train": TRAIN_CSV, "validation": VALID_CSV}
ds = load_dataset("csv", data_files=data_files)

# Cast audio column
ds = ds.cast_column("path", Audio(sampling_rate=16000))

# preprocess
def preprocess(batch):
    audio = batch["path"]["array"]
    # extract features
    inputs = processor(audio, sampling_rate=16000, return_tensors="pt")
    input_features = inputs.input_features[0]

    # tokenize labels
    with processor.as_target_processor():
        labels = processor(batch["text"]).input_ids

    return {"input_features": input_features, "labels": labels}

ds["train"] = ds["train"].map(preprocess)
ds["validation"] = ds["validation"].map(preprocess)

# collator
@dataclass
class DataCollatorSpeechSeq2Seq:
    processor: WhisperProcessor
    def __call__(self, features):
        input_features = torch.stack([torch.tensor(f["input_features"]) for f in features])
        labels = [f["labels"] for f in features]

        labels_batch = self.processor.tokenizer.pad(
            {"input_ids": labels}, padding=True, return_tensors="pt"
        ).input_ids
        labels_batch[labels_batch == self.processor.tokenizer.pad_token_id] = -100

        return {"input_features": input_features, "labels": labels_batch}

data_collator = DataCollatorSpeechSeq2Seq(processor)

# training args
training_args = Seq2SeqTrainingArguments(
    output_dir=OUTPUT_DIR,
    per_device_train_batch_size=BATCH_SIZE,
    per_device_eval_batch_size=BATCH_SIZE,
    gradient_accumulation_steps=4,
    fp16=True,
    num_train_epochs=EPOCHS,
    learning_rate=LEARNING_RATE,
    logging_steps=50,
    save_total_limit=2,
    evaluation_strategy="steps",
    eval_steps=200,
    save_steps=200,
    predict_with_generate=True,
    remove_unused_columns=False
)

# WER metric
wer_metric = evaluate.load("wer")

def compute_metrics(pred):
    label_ids = pred.label_ids
    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id
    pred_str = processor.tokenizer.batch_decode(pred.predictions, skip_special_tokens=True)
    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)
    return {"wer": wer_metric.compute(predictions=pred_str, references=label_str)}

# trainer
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=ds["train"],
    eval_dataset=ds["validation"],
    data_collator=data_collator,
    tokenizer=processor.feature_extractor,
    compute_metrics=compute_metrics,
)

if __name__ == "__main__":
    trainer.train()
    trainer.save_model(OUTPUT_DIR)








# train_whisper_v3.py
import os
import torch
import numpy as np
import soundfile as sf
from datasets import load_dataset
from transformers import (
    WhisperForConditionalGeneration,
    WhisperProcessor,
    Seq2SeqTrainingArguments,
    Seq2SeqTrainer
)
import evaluate
from dataclasses import dataclass
from typing import Any, Dict, List

# --- config ----------------------------------------------------------------------------
MODEL = "openai/whisper-large-v3"
DATASET_CSV = "data/manifest.csv"       # columns: path,text
OUTPUT_DIR = "whisper_ft_v3"
BATCH_SIZE = 4
EPOCHS = 3
LEARNING_RATE = 1e-5
# ---------------------------------------------------------------------------------------

processor = WhisperProcessor.from_pretrained(MODEL)
model = WhisperForConditionalGeneration.from_pretrained(MODEL)

# set language + task â†’ Báº®T BUá»˜C
model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(
    language="vi", task="transcribe"
)
model.config.suppress_tokens = []

model.to("cuda")

# load dataset
ds = load_dataset("csv", data_files={"train": DATASET_CSV})["train"]

# load audio
def load_audio(ex):
    speech, sr = sf.read(ex["path"])
    ex["audio"] = {"array": speech, "sampling_rate": sr}
    return ex

ds = ds.map(load_audio)

# preprocess fn
def preprocess(batch):
    audio = batch["audio"]

    # Whisper yÃªu cáº§u 16kHz
    if audio["sampling_rate"] != 16000:
        raise ValueError("Audio must be 16kHz. Resample first!")

    # extract log-mel
    features = processor(
        audio=audio["array"],
        sampling_rate=16000,
        return_tensors="pt"
    ).input_features[0]

    # tokenize text
    with processor.as_target_processor():
        labels = processor(batch["text"]).input_ids

    return {"input_features": features, "labels": labels}

ds = ds.map(preprocess, remove_columns=ds.column_names)

# collator
@dataclass
class DataCollatorSpeechSeq2Seq:
    processor: WhisperProcessor
    def __call__(self, features):
        input_features = torch.stack([torch.tensor(f["input_features"]) for f in features])
        labels = [f["labels"] for f in features]

        labels_batch = self.processor.tokenizer.pad(
            {"input_ids": labels}, 
            padding=True, 
            return_tensors="pt"
        ).input_ids

        labels_batch[labels_batch == self.processor.tokenizer.pad_token_id] = -100

        return {
            "input_features": input_features,
            "labels": labels_batch
        }

data_collator = DataCollatorSpeechSeq2Seq(processor)

# training args
training_args = Seq2SeqTrainingArguments(
    output_dir=OUTPUT_DIR,
    per_device_train_batch_size=BATCH_SIZE,
    gradient_accumulation_steps=4,
    fp16=True,
    num_train_epochs=EPOCHS,
    learning_rate=LEARNING_RATE,
    logging_steps=50,
    save_total_limit=2,
    evaluation_strategy="no",
    remove_unused_columns=False
)

# WER metric
wer = evaluate.load("wer")

def compute_metrics(pred):
    pred_ids = pred.predictions
    label_ids = pred.label_ids

    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id

    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)
    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)

    return {"wer": wer.compute(predictions=pred_str, references=label_str)}

# trainer
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=ds,
    data_collator=data_collator,
    tokenizer=processor.feature_extractor,
    compute_metrics=compute_metrics,
)

if __name__ == "__main__":
    trainer.train()
    trainer.save_model(OUTPUT_DIR)





import torch
from torch import nn
import inspect
from torch_geometric.nn.aggr import Aggregation
from torch_geometric.nn import MessagePassing
from torch_geometric.data import Data

# ---------------- MLPAutoencoder ----------------
class MLPAutoencoder(nn.Module):
    """
    Simple MLP Autoencoder: encode â†’ latent â†’ decode
    """
    def __init__(self, layer_sizes=(1, 2, 2, 4)):
        super().__init__()
        # Encoder
        enc_layers = []
        for i in range(len(layer_sizes) - 1):
            enc_layers.append(nn.Linear(layer_sizes[i], layer_sizes[i+1]))
            enc_layers.append(nn.ReLU())
        self.encoder = nn.Sequential(*enc_layers[:-1])  # remove last ReLU

        # Decoder (mirror of encoder)
        dec_layers = []
        rev_sizes = list(layer_sizes[::-1])
        for i in range(len(rev_sizes) - 1):
            dec_layers.append(nn.Linear(rev_sizes[i], rev_sizes[i+1]))
            dec_layers.append(nn.ReLU())
        self.decoder = nn.Sequential(*dec_layers[:-1])

    def forward(self, x):
        return self.encoder(x)

    def inverse(self, z):
        return self.decoder(z)

    def reset_parameters(self):
        for layer in self.encoder:
            if isinstance(layer, nn.Linear):
                nn.init.xavier_uniform_(layer.weight)
                nn.init.zeros_(layer.bias)
        for layer in self.decoder:
            if isinstance(layer, nn.Linear):
                nn.init.xavier_uniform_(layer.weight)
                nn.init.zeros_(layer.bias)


# ---------------- GenAgg ----------------
class GenAgg(Aggregation):
    """
    Generalized Aggregation using learnable MLP Autoencoder
    """
    def __init__(self, f=None, a=None, b=None, **kwargs):
        super().__init__()
        if f is None:
            self.f = MLPAutoencoder(layer_sizes=(1, 2, 2, 4))
        elif inspect.isclass(f):
            self.f = f(**kwargs)
        else:
            self.f = f

        self.a = nn.Parameter(torch.tensor(0.0)) if a is None else a
        self.b = nn.Parameter(torch.tensor(0.0)) if b is None else b

    def reset_parameters(self):
        nn.init.zeros_(self.a)
        nn.init.zeros_(self.b)
        if hasattr(self.f, 'reset_parameters'):
            self.f.reset_parameters()

    def get_n(self, x, index=None, ptr=None, dim_size=None, dim=-2):
        n = self.reduce(torch.ones_like(x), index, ptr, dim_size, dim, reduce='sum')
        n[n==0] = 1
        return n

    def forward(self, x, index=None, ptr=None, dim_size=None, dim=-2):
        if isinstance(self.b, nn.Parameter) or self.b != 0:
            x_mean = self.reduce(x, index, ptr, dim_size, dim, reduce='mean')
            if index is None:
                x = x - self.b * x_mean.unsqueeze(dim)
            else:
                x = x - self.b * torch.index_select(input=x_mean, dim=dim, index=index)

        n = self.get_n(x=x, index=index, ptr=ptr, dim_size=dim_size, dim=dim)
        x = x.unsqueeze(-1)
        n = n.unsqueeze(-1)
        if dim < 0:
            dim -= 1

        y1 = self.f.forward(x)
        y2 = self.reduce(y1, index, ptr, dim_size, dim, reduce='mean')
        y3 = y2 * (n**self.a)
        z = self.f.inverse(y3)
        z = z.squeeze(-1)

        return z

    def dist_op(self, a, b, type=1):
        if type == 1:
            return self.f.inverse(self.f.forward(a) * self.f.forward(b))
        elif type == 0:
            return self.f.inverse(self.f.forward(a) + self.f.forward(b))


# ---------------- Example: Message Passing GNN using GenAgg ----------------
class GenAggGNN(MessagePassing):
    def __init__(self, in_channels, out_channels):
        super().__init__(aggr=GenAgg())  # dÃ¹ng GenAgg aggregator
        self.lin = nn.Linear(in_channels, out_channels)

    def forward(self, x, edge_index):
        out = self.propagate(edge_index, x=x)
        out = self.lin(out)
        return out

    def message(self, x_j):
        return x_j

    def update(self, aggr_out):
        return aggr_out


# ---------------- Example usage ----------------
if __name__ == "__main__":
    # Create a small graph: 3 nodes, 4 edges
    edge_index = torch.tensor([[0, 1, 2, 0],
                               [1, 0, 0, 2]], dtype=torch.long)

    x = torch.randn(3, 5)  # 3 nodes, 5 features
    data = Data(x=x, edge_index=edge_index)

    # Initialize GNN
    model = GenAggGNN(in_channels=5, out_channels=4)

    # Forward
    out = model(data.x, data.edge_index)
    print("Output node features:", out)
    print("Shape:", out.shape)






import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader
from torch.utils.tensorboard import SummaryWriter
from captum.attr import IntegratedGradients
import matplotlib.pyplot as plt
import numpy as np
import random, os

# -----------------------------
# 1ï¸âƒ£ SETUP
# -----------------------------
torch.manual_seed(42)
random.seed(42)
np.random.seed(42)

input_dim = 20
hidden_dim = 32
num_classes = 3
num_samples = 500
batch_size = 32
epochs = 15
log_dir = "runs/model_interpret_full"

os.makedirs(log_dir, exist_ok=True)
writer = SummaryWriter(log_dir=log_dir)

# -----------------------------
# 2ï¸âƒ£ DATA: random cho dá»… nhÃ¬n
# -----------------------------
X = torch.randn(num_samples, 10, input_dim)  # (batch, seq, features)
y = torch.randint(0, num_classes, (num_samples,))

dataset = TensorDataset(X, y)
loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

# -----------------------------
# 3ï¸âƒ£ MODEL: LSTM cÆ¡ báº£n
# -----------------------------
class InterpretableLSTM(nn.Module):
    def __init__(self, input_dim, hidden_dim, num_classes):
        super().__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, num_classes)
        self.relu = nn.ReLU()

    def forward(self, x):
        h, _ = self.lstm(x)
        out = self.relu(h[:, -1, :])  # láº¥y hidden cuá»‘i
        logits = self.fc(out)
        return logits, {"hidden": h, "activation": out}

model = InterpretableLSTM(input_dim, hidden_dim, num_classes)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=1e-3)

# -----------------------------
# 4ï¸âƒ£ TRAIN LOOP + LOG
# -----------------------------
for epoch in range(epochs):
    model.train()
    total_loss, correct = 0, 0

    for batch_X, batch_y in loader:
        optimizer.zero_grad()
        logits, internals = model(batch_X)
        loss = criterion(logits, batch_y)
        loss.backward()

        optimizer.step()

        total_loss += loss.item()
        correct += (logits.argmax(1) == batch_y).sum().item()

        # Log histograms
        for name, param in model.named_parameters():
            writer.add_histogram(f"weights/{name}", param, epoch)
            if param.grad is not None:
                writer.add_histogram(f"grads/{name}", param.grad, epoch)

        # Log activation stats
        writer.add_histogram("activations/last_hidden", internals["activation"], epoch)

    acc = correct / len(dataset)
    avg_loss = total_loss / len(loader)
    writer.add_scalar("Loss/train", avg_loss, epoch)
    writer.add_scalar("Accuracy/train", acc, epoch)

    print(f"Epoch {epoch+1}/{epochs} | Loss: {avg_loss:.4f} | Acc: {acc:.3f}")

# -----------------------------
# 5ï¸âƒ£ EMBEDDING PROJECTOR
# -----------------------------
embeddings = []
labels = []
with torch.no_grad():
    for Xb, yb in loader:
        logits, internals = model(Xb)
        embeddings.append(internals["activation"])
        labels.append(yb)
embeddings = torch.cat(embeddings)
labels = torch.cat(labels)
writer.add_embedding(embeddings, metadata=labels, tag="embedding_space")

# -----------------------------
# 6ï¸âƒ£ FEATURE IMPORTANCE (Captum)
# -----------------------------
ig = IntegratedGradients(model)
model.eval()
sample_X = X[0:1]
sample_y = y[0:1]
attr, delta = ig.attribute(sample_X, target=sample_y.item(), return_convergence_delta=True)

# Váº½ heatmap thá»ƒ hiá»‡n importance theo chiá»u thá»i gian + feature
attr_np = attr.squeeze(0).detach().numpy()
plt.figure(figsize=(8, 4))
plt.imshow(attr_np.T, cmap="hot", aspect="auto")
plt.colorbar(label="Feature importance")
plt.title("Integrated Gradients Heatmap (Captum)")
plt.xlabel("Time step")
plt.ylabel("Feature dim")
plt.tight_layout()
plt.savefig(os.path.join(log_dir, "feature_importance_heatmap.png"))
plt.close()

writer.close()
print("âœ… Training done. Logs saved to:", log_dir)
print("Run TensorBoard: tensorboard --logdir runs/model_interpret_full")









import torch
import matplotlib.pyplot as plt

# ============================================================
# 1ï¸âƒ£ Táº¡o ma tráº­n HiPPO-LegS (A, B)
# ============================================================
def hippo_legs_matrix(N: int):
    """Táº¡o ma tráº­n A, B cho HiPPO-LegS"""
    n = torch.arange(N, dtype=torch.float32)
    A = torch.zeros((N, N))
    for i in range(N):
        for j in range(N):
            if j <= i:
                A[i, j] = -(2 * i + 1) * ((-1) ** (i - j))
    B = (2 * n + 1).sqrt().unsqueeze(-1)
    return A, B

# ============================================================
# 2ï¸âƒ£ Rá»i ráº¡c hÃ³a báº±ng Bilinear transform (Tustin)
# ============================================================
def bilinear_discretize(A, B, dt=0.01):
    I = torch.eye(A.size(0))
    Ad = (I + 0.5 * dt * A) @ torch.linalg.inv(I - 0.5 * dt * A)
    Bd = torch.linalg.inv(I - 0.5 * dt * A) @ B * dt
    return Ad, Bd

# ============================================================
# 3ï¸âƒ£ MÃ´ phá»ng HiPPO vá»›i tÃ­n hiá»‡u x(t) = t
# ============================================================
def simulate_hippo(N=4, T=1.0, dt=0.01):
    steps = int(T / dt)
    A, B = hippo_legs_matrix(N)
    Ad, Bd = bilinear_discretize(A, B, dt)

    c = torch.zeros(N, 1)
    states = [c.clone()]
    xs = []
    ts = []

    for step in range(steps):
        t = step * dt
        x_t = torch.tensor([t])  # x(t) = t
        c = Ad @ c + Bd * x_t
        states.append(c.clone())
        xs.append(x_t.item())
        ts.append(t)

    states = torch.cat(states, dim=1)
    return ts, xs, states

# ============================================================
# 4ï¸âƒ£ Váº½ káº¿t quáº£
# ============================================================
ts, xs, states = simulate_hippo(N=4, T=1.0, dt=0.01)

plt.figure(figsize=(10, 6))
for i in range(states.size(0)):
    plt.plot(ts, states[i, 1:].numpy(), label=f'c[{i}]')

plt.title("Tiáº¿n hÃ³a há»‡ sá»‘ HiPPO-LegS cho x(t)=t")
plt.xlabel("Thá»i gian t")
plt.ylabel("GiÃ¡ trá»‹ há»‡ sá»‘ c_i(t)")
plt.legend()
plt.grid(True)
plt.show()





import pandas as pd
import numpy as np

INPUT = "NF-ToN-IoT-v2_1M_balanced.csv"
OUTPUT = "NF-ToN-IoT-v2_1M_balanced_shuffled.csv"
CHUNK_SIZE = 100_000
RANDOM_STATE = 1234

# 1. Äá»c file thÃ nh tá»«ng khá»‘i
chunks = pd.read_csv(INPUT, chunksize=CHUNK_SIZE)

# 2. Shuffle thá»© tá»± cÃ¡c khá»‘i
chunks = list(chunks)
np.random.seed(RANDOM_STATE)
np.random.shuffle(chunks)

# 3. Shuffle trong tá»«ng khá»‘i (Ã­t tá»‘n RAM)
for i, chunk in enumerate(chunks):
    chunk = chunk.sample(frac=1.0, random_state=RANDOM_STATE + i)
    mode = "w" if i == 0 else "a"
    header = (i == 0)
    chunk.to_csv(OUTPUT, index=False, mode=mode, header=header)

print("âœ… ÄÃ£ shuffle toÃ n bá»™ vÃ  lÆ°u:", OUTPUT)




import pandas as pd
import numpy as np

# ==============================================================
# âš™ï¸ Cáº¤U HÃŒNH
# ==============================================================
INPUT = "NF-ToN-IoT-v2_100k.csv"           # file Ä‘áº§u vÃ o
OUTPUT = "NF-ToN-IoT-v2_100k_clean.csv"    # file sau khi lÃ m sáº¡ch
CLIP_MIN, CLIP_MAX = -1e6, 1e6             # giá»›i háº¡n giÃ¡ trá»‹ cá»±c trá»‹

# ==============================================================
# 1ï¸âƒ£ Äá»c dá»¯ liá»‡u
# ==============================================================
df = pd.read_csv(INPUT)
print("ðŸ“Š KÃ­ch thÆ°á»›c ban Ä‘áº§u:", df.shape)

if 'Attack' not in df.columns:
    raise ValueError("âŒ KhÃ´ng tÃ¬m tháº¥y cá»™t 'Attack' trong file CSV.")

# ==============================================================
# 2ï¸âƒ£ Xá»­ lÃ½ NaN
# ==============================================================
# XoÃ¡ dÃ²ng khÃ´ng cÃ³ nhÃ£n Attack
df = df.dropna(subset=['Attack']).reset_index(drop=True)

# XÃ¡c Ä‘á»‹nh cá»™t sá»‘ vÃ  cá»™t chuá»—i
num_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()
cat_cols = df.select_dtypes(include=['object']).columns.drop('Attack').tolist()

# Äiá»n giÃ¡ trá»‹ thiáº¿u
for col in num_cols:
    df[col] = df[col].fillna(df[col].median())

for col in cat_cols:
    df[col] = df[col].fillna(df[col].mode()[0])

print(f"âœ… ÄÃ£ xá»­ lÃ½ NaN: {len(num_cols)} cá»™t sá»‘, {len(cat_cols)} cá»™t chuá»—i.")

# ==============================================================
# 3ï¸âƒ£ Giá»›i háº¡n (clip) giÃ¡ trá»‹ cá»±c trá»‹
# ==============================================================
df[num_cols] = df[num_cols].clip(lower=CLIP_MIN, upper=CLIP_MAX)
print(f"âœ… ÄÃ£ giá»›i háº¡n giÃ¡ trá»‹ trong khoáº£ng [{CLIP_MIN}, {CLIP_MAX}]")

# ==============================================================
# 4ï¸âƒ£ LÆ°u dá»¯ liá»‡u sáº¡ch
# ==============================================================
df.to_csv(OUTPUT, index=False)
print("âœ… ÄÃ£ lÆ°u file lÃ m sáº¡ch:", OUTPUT)





import pandas as pd
from sklearn.utils import resample

INPUT = "NF-ToN-IoT-v2.csv"
OUTPUT = "NF-ToN-IoT-v2_1M_balanced.csv"
TARGET_TOTAL = 1_000_000
RANDOM_STATE = 1234

df = pd.read_csv(INPUT)

if 'Attack' not in df.columns:
    raise ValueError("File CSV khÃ´ng cÃ³ cá»™t 'Attack'")

classes = df['Attack'].unique()
num_classes = len(classes)
target_per_class = TARGET_TOTAL // num_classes
remainder = TARGET_TOTAL % num_classes

sampled_parts = []
for i, cls in enumerate(sorted(classes)):
    cls_df = df[df['Attack'] == cls]
    this_target = target_per_class + (1 if i < remainder else 0)
    part = cls_df.sample(
        n=this_target,
        replace=(len(cls_df) < this_target),
        random_state=RANDOM_STATE,
    ).sample(frac=1.0, random_state=RANDOM_STATE)  # shuffle riÃªng tá»«ng lá»›p
    sampled_parts.append(part)

# concat mÃ  khÃ´ng shuffle toÃ n cá»¥c
df_balanced = pd.concat(sampled_parts, ignore_index=True)

print(df_balanced['Attack'].value_counts())

df_balanced.to_csv(OUTPUT, index=False)
print("Saved to:", OUTPUT)





