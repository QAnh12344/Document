import torch
import torch.nn as nn
from torch_geometric.nn import HypergraphConv

# Example HyperGCN model
class HyperGCN(nn.Module):
    def __init__(self, in_dim, hid_dim, out_dim):
        super().__init__()
        self.conv1 = HypergraphConv(in_dim, hid_dim)
        self.conv2 = HypergraphConv(hid_dim, out_dim)

    def forward(self, x, hyperedge_index):
        x = self.conv1(x, hyperedge_index)
        x = torch.relu(x)
        x = self.conv2(x, hyperedge_index)
        return x

# Example training loop

def train(model, data, epochs=200, lr=1e-3):
    model.train()
    opt = torch.optim.Adam(model.parameters(), lr=lr)
    loss_fn = nn.CrossEntropyLoss()

    x = data.x
    y = data.y
    hyperedge_index = data.hyperedge_index
    train_mask = data.train_mask

    for epoch in range(epochs):
        opt.zero_grad()
        out = model(x, hyperedge_index)
        loss = loss_fn(out[train_mask], y[train_mask])
        loss.backward()
        opt.step()

        if epoch % 20 == 0:
            print(f"Epoch {epoch} | Loss: {loss.item():.4f}")

# Usage:
# from torch_geometric.data import Data
# data = Data(x=node_features, y=labels, hyperedge_index=hyperedge_index, train_mask=train_mask)
# model = HyperGCN(in_dim=x.size(1), hid_dim=64, out_dim=num_classes)
# train(model, data)

# Added: test loop and full train+test pipeline

def test(model, data):
    model.eval()
    x, hyperedge_index = data.x, data.hyperedge_index
    out = model(x, hyperedge_index)
    pred = out.argmax(dim=1)
    acc = (pred[data.test_mask] == data.y[data.test_mask]).float().mean().item()
    print(f"Test Accuracy: {acc:.4f}")
    return acc

# Example full usage:
# data = Data(x=x, y=y, hyperedge_index=edge_index, train_mask=train_mask, val_mask=val_mask, test_mask=test_mask)
# model = HyperGCN(in_dim=x.size(1), hid_dim=64, out_dim=num_classes)
# train(model, data, epochs=200)
# test(model, data)

# ------------------ HyperGCL-style Contrastive + Supervised Training ------------------
import torch.nn.functional as F
from torch_geometric.nn import global_mean_pool


def nt_xent_loss(z1: Tensor, z2: Tensor, temperature: float = 0.5) -> Tensor:
    """Normalized temperature-scaled cross entropy loss for sets of graph embeddings.
    z1, z2: [batch, dim] (here batch = number of graphs/views; for node-level contrastive you would adapt)
    We compute contrastive loss pairing i in z1 with i in z2 (positive), others negative.
    """
    z1 = F.normalize(z1, p=2, dim=1)
    z2 = F.normalize(z2, p=2, dim=1)
    batch_size = z1.size(0)

    representations = torch.cat([z1, z2], dim=0)  # [2B, D]
    sim_matrix = torch.matmul(representations, representations.t())  # [2B,2B]
    sim_matrix = sim_matrix / temperature

    # mask out self-similarities
    mask = (~torch.eye(2 * batch_size, dtype=torch.bool, device=sim_matrix.device)).float()
    exp_sim = torch.exp(sim_matrix) * mask

    # positive pairs: i with i+B and i+B with i
    pos_sim = torch.exp(torch.sum(z1 * z2, dim=-1) / temperature)
    pos = torch.cat([pos_sim, pos_sim], dim=0)

    denom = exp_sim.sum(dim=1)
    loss = -torch.log(pos / denom)
    return loss.mean()


class HyperGCLTrainer:
    def __init__(self, model, aug_module, lr=1e-3, temperature=0.5, alpha=1.0, device='cpu'):
        """alpha: weight for supervised loss; contrastive weight = 1.0 by default"""
        self.model = model.to(device)
        self.aug = aug_module
        self.opt = torch.optim.Adam(self.model.parameters(), lr=lr)
        self.temperature = temperature
        self.alpha = alpha
        self.device = device

    def graph_repr(self, data: Data) -> Tensor:
        """Get graph-level representation by mean pooling node embeddings.
        Assumes single hypergraph per Data object.
        """
        self.model.eval()
        x = data.x.to(self.device)
        he = data.hyperedge_index.to(self.device)
        h = self.model(x, he)  # node embeddings
        # create a fake batch index all zeros (single graph) when pooling
        batch_index = torch.zeros(h.size(0), dtype=torch.long, device=h.device)
        g = global_mean_pool(h, batch_index)
        return h, g

    def train_one_epoch(self, data: Data, aug_params1, aug_params2):
        self.model.train()
        # create two augmented views
        v1, v2 = self.aug.make_two_views(data, aug_params1, aug_params2)
        # move to device
        v1.x = v1.x.to(self.device); v1.hyperedge_index = v1.hyperedge_index.to(self.device)
        v2.x = v2.x.to(self.device); v2.hyperedge_index = v2.hyperedge_index.to(self.device)

        # forward
        h1 = self.model(v1.x, v1.hyperedge_index)
        h2 = self.model(v2.x, v2.hyperedge_index)

        # graph-level embeddings (mean pool)
        batch1 = torch.zeros(h1.size(0), dtype=torch.long, device=h1.device)
        batch2 = torch.zeros(h2.size(0), dtype=torch.long, device=h2.device)
        g1 = global_mean_pool(h1, batch1)
        g2 = global_mean_pool(h2, batch2)

        # contrastive loss (between g1 and g2)
        contrastive_loss = nt_xent_loss(g1, g2, temperature=self.temperature)

        # supervised loss on original nodes (use original data labels)
        out = self.model(data.x.to(self.device), data.hyperedge_index.to(self.device))
        sup_loss = F.cross_entropy(out[data.train_mask.to(self.device)], data.y[data.train_mask].to(self.device))

        loss = contrastive_loss + self.alpha * sup_loss

        self.opt.zero_grad()
        loss.backward()
        self.opt.step()

        return loss.item(), contrastive_loss.item(), sup_loss.item()

    def validate(self, data: Data):
        self.model.eval()
        out = self.model(data.x.to(self.device), data.hyperedge_index.to(self.device))
        preds = out.argmax(dim=1)
        val_acc = (preds[data.val_mask.to(self.device)] == data.y[data.val_mask].to(self.device)).float().mean().item()
        return val_acc

    def test(self, data: Data):
        self.model.eval()
        out = self.model(data.x.to(self.device), data.hyperedge_index.to(self.device))
        preds = out.argmax(dim=1)
        test_acc = (preds[data.test_mask.to(self.device)] == data.y[data.test_mask].to(self.device)).float().mean().item()
        return test_acc


def run_training(model, data, aug_module, epochs=200, lr=1e-3, temperature=0.5, alpha=1.0, device='cpu'):
    trainer = HyperGCLTrainer(model, aug_module, lr=lr, temperature=temperature, alpha=alpha, device=device)

    # example augmentation pipelines (you can customize)
    aug1 = [('node_drop', {'drop_prob':0.2}), ('feature_mask', {'mask_prob':0.1})]
    aug2 = [('hyperedge_drop', {'drop_prob':0.3}), ('hyperedge_perturbation', {'add_ratio':0.05, 'remove_ratio':0.1})]

    best_val = 0.0
    best_model_state = None
    for epoch in range(1, epochs + 1):
        loss, cl_loss, sup_loss = trainer.train_one_epoch(data, aug1, aug2)
        if epoch % 10 == 0:
            val_acc = trainer.validate(data)
            print(f"Epoch {epoch:03d} | Loss {loss:.4f} | CL {cl_loss:.4f} | Sup {sup_loss:.4f} | ValAcc {val_acc:.4f}")
            if val_acc > best_val:
                best_val = val_acc
                best_model_state = {k: v.cpu() for k, v in trainer.model.state_dict().items()}

    # load best model and test
    if best_model_state is not None:
        trainer.model.load_state_dict(best_model_state)
    test_acc = trainer.test(data)
    print(f"Test Accuracy: {test_acc:.4f} | Best Val: {best_val:.4f}")
    return trainer.model

# End of HyperGCL-style training pipeline






import torch
from torch_geometric.data import HeteroData

# ==========================
# 1ï¸âƒ£ Táº¡o HeteroGraphData vá»›i multi-edges
# ==========================
data = HeteroData()

# Node features
data['node'].x = torch.tensor([[1.0, 0.5],  # node 0
                               [0.2, 1.5],  # node 1
                               [0.0, 0.1]]) # node 2

# Multi-edges: 0->1 (3 edges), 1->2 (2 edges)
edge_index = torch.tensor([
    [0, 0, 0, 1, 1],  # src
    [1, 1, 1, 2, 2]   # dst
])
edge_attr = torch.tensor([
    [1.0, 0.0],  # edge 0
    [0.5, 0.5],  # edge 1
    [0.0, 1.0],  # edge 2
    [1.0, 1.0],  # edge 3
    [0.0, 2.0]   # edge 4
])

data['node', 'to', 'node'].edge_index = edge_index
data['node', 'to', 'node'].edge_attr = edge_attr

# Reverse edges
data['node', 'rev_to', 'node'].edge_index = edge_index.flipud()
data['node', 'rev_to', 'node'].edge_attr = edge_attr.clone()

# ==========================
# 2ï¸âƒ£ Flatten edge mapping
# ==========================
def find_parallel_edges(edge_index):
    simplified_edge_mapping = {}
    simplified_edge_batch = []
    i = 0
    for edge in edge_index.T:
        tuple_edge = tuple(edge.tolist())
        if tuple_edge not in simplified_edge_mapping:
            simplified_edge_mapping[tuple_edge] = i
            i += 1
        simplified_edge_batch.append(simplified_edge_mapping[tuple_edge])
    return torch.LongTensor(simplified_edge_batch)

simp_edge_batch = find_parallel_edges(edge_index)

# GÃ¡n vÃ o graph
data['node', 'to', 'node'].simp_edge_batch = simp_edge_batch
data['node', 'rev_to', 'node'].simp_edge_batch = simp_edge_batch

print("simp_edge_batch:", simp_edge_batch)

# ==========================
# 3ï¸âƒ£ Flatten edge features (sum pooling)
# ==========================
def flatten_edge_features(edge_attr, simp_edge_batch):
    num_flattened_edges = simp_edge_batch.max().item() + 1
    flattened_edge_attr = torch.zeros((num_flattened_edges, edge_attr.size(1)))
    for i in range(num_flattened_edges):
        mask = simp_edge_batch == i
        flattened_edge_attr[i] = edge_attr[mask].sum(dim=0)
    return flattened_edge_attr

flat_attr_to = flatten_edge_features(data['node', 'to', 'node'].edge_attr,
                                     data['node', 'to', 'node'].simp_edge_batch)
flat_attr_rev = flatten_edge_features(data['node', 'rev_to', 'node'].edge_attr,
                                     data['node', 'rev_to', 'node'].simp_edge_batch)

print("Flattened edge features (to):\n", flat_attr_to)
print("Flattened edge features (rev):\n", flat_attr_rev)

# ==========================
# 4ï¸âƒ£ Káº¿t quáº£
# ==========================
# Edge 0->1: 3 multi-edges -> sum
# Edge 1->2: 2 multi-edges -> sum




import re

def analyze_text(text,
                 max_numbers=3,
                 max_english_words=5,
                 max_brackets=4):
    """
    Kiá»ƒm tra cÃ¢u xem cÃ³ quÃ¡ nhiá»u sá»‘, tá»« tiáº¿ng Anh, hoáº·c dáº¥u ngoáº·c.
    """
    # ---- Äáº¿m sá»‘ ----
    numbers = re.findall(r'\d+(\.\d+)?', text)
    num_count = len(numbers)

    # ---- Äáº¿m tá»« tiáº¿ng Anh ----
    english_words = re.findall(r'\b[a-zA-Z]+\b', text)
    eng_count = len(english_words)

    # ---- Äáº¿m dáº¥u ngoáº·c ----
    brackets = re.findall(r'[()\[\]{}]', text)
    br_count = len(brackets)

    # ---- Táº¡o thÃ´ng bÃ¡o ----
    messages = []

    if num_count > max_numbers:
        messages.append("QuÃ¡ nhiá»u sá»‘")

    if eng_count > max_english_words:
        messages.append("QuÃ¡ nhiá»u tá»« tiáº¿ng Anh")

    if br_count > max_brackets:
        messages.append("QuÃ¡ nhiá»u dáº¥u ngoáº·c")

    # Náº¿u khÃ´ng cÃ³ gÃ¬ báº¥t thÆ°á»ng
    if not messages:
        return "BÃ¬nh thÆ°á»ng"

    # GhÃ©p thÃ´ng bÃ¡o
    return ", ".join(messages)


# ---- VÃ­ dá»¥ ----
texts = [
    "GiÃ¡ lÃ  3 (láº§n), thÃªm 2 (option) vÃ  5 [more] items",
    "This is a test (a) (b) (c) (d) (e)",
    "HÃ´m nay cÃ³ 2 quáº£ tÃ¡o thÃ´i",
    "I bought 3 apples, 2 bananas and {one} more item"
]

for t in texts:
    print(f"'{t}' -> {analyze_text(t)}")





import re

def analyze_text(text, max_numbers=3, max_english_words=5):
    """
    Kiá»ƒm tra má»™t cÃ¢u xem cÃ³ quÃ¡ nhiá»u sá»‘ hoáº·c quÃ¡ nhiá»u tá»« tiáº¿ng Anh khÃ´ng.
    
    Args:
        text (str): CÃ¢u cáº§n kiá»ƒm tra
        max_numbers (int): NgÆ°á»¡ng tá»‘i Ä‘a sá»‘ lÆ°á»£ng sá»‘
        max_english_words (int): NgÆ°á»¡ng tá»‘i Ä‘a sá»‘ lÆ°á»£ng tá»« tiáº¿ng Anh
    
    Returns:
        str: ThÃ´ng bÃ¡o tÃ¬nh tráº¡ng cÃ¢u
    """
    # Äáº¿m sá»‘
    numbers = re.findall(r'\d+(\.\d+)?', text)
    num_count = len(numbers)
    
    # Äáº¿m tá»« tiáº¿ng Anh
    english_words = re.findall(r'\b[a-zA-Z]+\b', text)
    eng_count = len(english_words)
    
    # Kiá»ƒm tra ngÆ°á»¡ng
    if num_count > max_numbers and eng_count > max_english_words:
        return "QuÃ¡ nhiá»u sá»‘ vÃ  quÃ¡ nhiá»u tá»« tiáº¿ng Anh"
    elif num_count > max_numbers:
        return "QuÃ¡ nhiá»u sá»‘"
    elif eng_count > max_english_words:
        return "QuÃ¡ nhiá»u tá»« tiáº¿ng Anh"
    else:
        return "BÃ¬nh thÆ°á»ng"

# VÃ­ dá»¥ sá»­ dá»¥ng
texts = [
    "HÃ´m nay tÃ´i mua 3 apples, 2 oranges vÃ  5 bananas giÃ¡ 10 USD",
    "Chá»‰ cÃ³ 2 quáº£ tÃ¡o",
    "I love Python programming and AI"
]

for t in texts:
    print(f"'{t}' -> {analyze_text(t)}")





# train_whisper_v3.py
import os
import torch
import numpy as np
import soundfile as sf
from datasets import load_dataset, Audio
from transformers import (
    WhisperForConditionalGeneration,
    WhisperProcessor,
    Seq2SeqTrainingArguments,
    Seq2SeqTrainer
)
import evaluate
from dataclasses import dataclass
from typing import Any, Dict, List

# --- config ----------------------------------------------------------------------------
MODEL = "openai/whisper-large-v3"
TRAIN_CSV = "dataset_whisper/train.csv"
VALID_CSV = "dataset_whisper/valid.csv"
OUTPUT_DIR = "whisper_ft_v3"
BATCH_SIZE = 4
EPOCHS = 3
LEARNING_RATE = 1e-5
# ---------------------------------------------------------------------------------------

# processor + model
processor = WhisperProcessor.from_pretrained(MODEL)
model = WhisperForConditionalGeneration.from_pretrained(MODEL)

# set language + task
model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(
    language="vi", task="transcribe"
)
model.config.suppress_tokens = []

model.to("cuda")

# load datasets
data_files = {"train": TRAIN_CSV, "validation": VALID_CSV}
ds = load_dataset("csv", data_files=data_files)

# Cast audio column
ds = ds.cast_column("path", Audio(sampling_rate=16000))

# preprocess
def preprocess(batch):
    audio = batch["path"]["array"]
    # extract features
    inputs = processor(audio, sampling_rate=16000, return_tensors="pt")
    input_features = inputs.input_features[0]

    # tokenize labels
    with processor.as_target_processor():
        labels = processor(batch["text"]).input_ids

    return {"input_features": input_features, "labels": labels}

ds["train"] = ds["train"].map(preprocess)
ds["validation"] = ds["validation"].map(preprocess)

# collator
@dataclass
class DataCollatorSpeechSeq2Seq:
    processor: WhisperProcessor
    def __call__(self, features):
        input_features = torch.stack([torch.tensor(f["input_features"]) for f in features])
        labels = [f["labels"] for f in features]

        labels_batch = self.processor.tokenizer.pad(
            {"input_ids": labels}, padding=True, return_tensors="pt"
        ).input_ids
        labels_batch[labels_batch == self.processor.tokenizer.pad_token_id] = -100

        return {"input_features": input_features, "labels": labels_batch}

data_collator = DataCollatorSpeechSeq2Seq(processor)

# training args
training_args = Seq2SeqTrainingArguments(
    output_dir=OUTPUT_DIR,
    per_device_train_batch_size=BATCH_SIZE,
    per_device_eval_batch_size=BATCH_SIZE,
    gradient_accumulation_steps=4,
    fp16=True,
    num_train_epochs=EPOCHS,
    learning_rate=LEARNING_RATE,
    logging_steps=50,
    save_total_limit=2,
    evaluation_strategy="steps",
    eval_steps=200,
    save_steps=200,
    predict_with_generate=True,
    remove_unused_columns=False
)

# WER metric
wer_metric = evaluate.load("wer")

def compute_metrics(pred):
    label_ids = pred.label_ids
    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id
    pred_str = processor.tokenizer.batch_decode(pred.predictions, skip_special_tokens=True)
    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)
    return {"wer": wer_metric.compute(predictions=pred_str, references=label_str)}

# trainer
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=ds["train"],
    eval_dataset=ds["validation"],
    data_collator=data_collator,
    tokenizer=processor.feature_extractor,
    compute_metrics=compute_metrics,
)

if __name__ == "__main__":
    trainer.train()
    trainer.save_model(OUTPUT_DIR)








# train_whisper_v3.py
import os
import torch
import numpy as np
import soundfile as sf
from datasets import load_dataset
from transformers import (
    WhisperForConditionalGeneration,
    WhisperProcessor,
    Seq2SeqTrainingArguments,
    Seq2SeqTrainer
)
import evaluate
from dataclasses import dataclass
from typing import Any, Dict, List

# --- config ----------------------------------------------------------------------------
MODEL = "openai/whisper-large-v3"
DATASET_CSV = "data/manifest.csv"       # columns: path,text
OUTPUT_DIR = "whisper_ft_v3"
BATCH_SIZE = 4
EPOCHS = 3
LEARNING_RATE = 1e-5
# ---------------------------------------------------------------------------------------

processor = WhisperProcessor.from_pretrained(MODEL)
model = WhisperForConditionalGeneration.from_pretrained(MODEL)

# set language + task â†’ Báº®T BUá»˜C
model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(
    language="vi", task="transcribe"
)
model.config.suppress_tokens = []

model.to("cuda")

# load dataset
ds = load_dataset("csv", data_files={"train": DATASET_CSV})["train"]

# load audio
def load_audio(ex):
    speech, sr = sf.read(ex["path"])
    ex["audio"] = {"array": speech, "sampling_rate": sr}
    return ex

ds = ds.map(load_audio)

# preprocess fn
def preprocess(batch):
    audio = batch["audio"]

    # Whisper yÃªu cáº§u 16kHz
    if audio["sampling_rate"] != 16000:
        raise ValueError("Audio must be 16kHz. Resample first!")

    # extract log-mel
    features = processor(
        audio=audio["array"],
        sampling_rate=16000,
        return_tensors="pt"
    ).input_features[0]

    # tokenize text
    with processor.as_target_processor():
        labels = processor(batch["text"]).input_ids

    return {"input_features": features, "labels": labels}

ds = ds.map(preprocess, remove_columns=ds.column_names)

# collator
@dataclass
class DataCollatorSpeechSeq2Seq:
    processor: WhisperProcessor
    def __call__(self, features):
        input_features = torch.stack([torch.tensor(f["input_features"]) for f in features])
        labels = [f["labels"] for f in features]

        labels_batch = self.processor.tokenizer.pad(
            {"input_ids": labels}, 
            padding=True, 
            return_tensors="pt"
        ).input_ids

        labels_batch[labels_batch == self.processor.tokenizer.pad_token_id] = -100

        return {
            "input_features": input_features,
            "labels": labels_batch
        }

data_collator = DataCollatorSpeechSeq2Seq(processor)

# training args
training_args = Seq2SeqTrainingArguments(
    output_dir=OUTPUT_DIR,
    per_device_train_batch_size=BATCH_SIZE,
    gradient_accumulation_steps=4,
    fp16=True,
    num_train_epochs=EPOCHS,
    learning_rate=LEARNING_RATE,
    logging_steps=50,
    save_total_limit=2,
    evaluation_strategy="no",
    remove_unused_columns=False
)

# WER metric
wer = evaluate.load("wer")

def compute_metrics(pred):
    pred_ids = pred.predictions
    label_ids = pred.label_ids

    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id

    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)
    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)

    return {"wer": wer.compute(predictions=pred_str, references=label_str)}

# trainer
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=ds,
    data_collator=data_collator,
    tokenizer=processor.feature_extractor,
    compute_metrics=compute_metrics,
)

if __name__ == "__main__":
    trainer.train()
    trainer.save_model(OUTPUT_DIR)





import torch
from torch import nn
import inspect
from torch_geometric.nn.aggr import Aggregation
from torch_geometric.nn import MessagePassing
from torch_geometric.data import Data

# ---------------- MLPAutoencoder ----------------
class MLPAutoencoder(nn.Module):
    """
    Simple MLP Autoencoder: encode â†’ latent â†’ decode
    """
    def __init__(self, layer_sizes=(1, 2, 2, 4)):
        super().__init__()
        # Encoder
        enc_layers = []
        for i in range(len(layer_sizes) - 1):
            enc_layers.append(nn.Linear(layer_sizes[i], layer_sizes[i+1]))
            enc_layers.append(nn.ReLU())
        self.encoder = nn.Sequential(*enc_layers[:-1])  # remove last ReLU

        # Decoder (mirror of encoder)
        dec_layers = []
        rev_sizes = list(layer_sizes[::-1])
        for i in range(len(rev_sizes) - 1):
            dec_layers.append(nn.Linear(rev_sizes[i], rev_sizes[i+1]))
            dec_layers.append(nn.ReLU())
        self.decoder = nn.Sequential(*dec_layers[:-1])

    def forward(self, x):
        return self.encoder(x)

    def inverse(self, z):
        return self.decoder(z)

    def reset_parameters(self):
        for layer in self.encoder:
            if isinstance(layer, nn.Linear):
                nn.init.xavier_uniform_(layer.weight)
                nn.init.zeros_(layer.bias)
        for layer in self.decoder:
            if isinstance(layer, nn.Linear):
                nn.init.xavier_uniform_(layer.weight)
                nn.init.zeros_(layer.bias)


# ---------------- GenAgg ----------------
class GenAgg(Aggregation):
    """
    Generalized Aggregation using learnable MLP Autoencoder
    """
    def __init__(self, f=None, a=None, b=None, **kwargs):
        super().__init__()
        if f is None:
            self.f = MLPAutoencoder(layer_sizes=(1, 2, 2, 4))
        elif inspect.isclass(f):
            self.f = f(**kwargs)
        else:
            self.f = f

        self.a = nn.Parameter(torch.tensor(0.0)) if a is None else a
        self.b = nn.Parameter(torch.tensor(0.0)) if b is None else b

    def reset_parameters(self):
        nn.init.zeros_(self.a)
        nn.init.zeros_(self.b)
        if hasattr(self.f, 'reset_parameters'):
            self.f.reset_parameters()

    def get_n(self, x, index=None, ptr=None, dim_size=None, dim=-2):
        n = self.reduce(torch.ones_like(x), index, ptr, dim_size, dim, reduce='sum')
        n[n==0] = 1
        return n

    def forward(self, x, index=None, ptr=None, dim_size=None, dim=-2):
        if isinstance(self.b, nn.Parameter) or self.b != 0:
            x_mean = self.reduce(x, index, ptr, dim_size, dim, reduce='mean')
            if index is None:
                x = x - self.b * x_mean.unsqueeze(dim)
            else:
                x = x - self.b * torch.index_select(input=x_mean, dim=dim, index=index)

        n = self.get_n(x=x, index=index, ptr=ptr, dim_size=dim_size, dim=dim)
        x = x.unsqueeze(-1)
        n = n.unsqueeze(-1)
        if dim < 0:
            dim -= 1

        y1 = self.f.forward(x)
        y2 = self.reduce(y1, index, ptr, dim_size, dim, reduce='mean')
        y3 = y2 * (n**self.a)
        z = self.f.inverse(y3)
        z = z.squeeze(-1)

        return z

    def dist_op(self, a, b, type=1):
        if type == 1:
            return self.f.inverse(self.f.forward(a) * self.f.forward(b))
        elif type == 0:
            return self.f.inverse(self.f.forward(a) + self.f.forward(b))


# ---------------- Example: Message Passing GNN using GenAgg ----------------
class GenAggGNN(MessagePassing):
    def __init__(self, in_channels, out_channels):
        super().__init__(aggr=GenAgg())  # dÃ¹ng GenAgg aggregator
        self.lin = nn.Linear(in_channels, out_channels)

    def forward(self, x, edge_index):
        out = self.propagate(edge_index, x=x)
        out = self.lin(out)
        return out

    def message(self, x_j):
        return x_j

    def update(self, aggr_out):
        return aggr_out


# ---------------- Example usage ----------------
if __name__ == "__main__":
    # Create a small graph: 3 nodes, 4 edges
    edge_index = torch.tensor([[0, 1, 2, 0],
                               [1, 0, 0, 2]], dtype=torch.long)

    x = torch.randn(3, 5)  # 3 nodes, 5 features
    data = Data(x=x, edge_index=edge_index)

    # Initialize GNN
    model = GenAggGNN(in_channels=5, out_channels=4)

    # Forward
    out = model(data.x, data.edge_index)
    print("Output node features:", out)
    print("Shape:", out.shape)






import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader
from torch.utils.tensorboard import SummaryWriter
from captum.attr import IntegratedGradients
import matplotlib.pyplot as plt
import numpy as np
import random, os

# -----------------------------
# 1ï¸âƒ£ SETUP
# -----------------------------
torch.manual_seed(42)
random.seed(42)
np.random.seed(42)

input_dim = 20
hidden_dim = 32
num_classes = 3
num_samples = 500
batch_size = 32
epochs = 15
log_dir = "runs/model_interpret_full"

os.makedirs(log_dir, exist_ok=True)
writer = SummaryWriter(log_dir=log_dir)

# -----------------------------
# 2ï¸âƒ£ DATA: random cho dá»… nhÃ¬n
# -----------------------------
X = torch.randn(num_samples, 10, input_dim)  # (batch, seq, features)
y = torch.randint(0, num_classes, (num_samples,))

dataset = TensorDataset(X, y)
loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

# -----------------------------
# 3ï¸âƒ£ MODEL: LSTM cÆ¡ báº£n
# -----------------------------
class InterpretableLSTM(nn.Module):
    def __init__(self, input_dim, hidden_dim, num_classes):
        super().__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, num_classes)
        self.relu = nn.ReLU()

    def forward(self, x):
        h, _ = self.lstm(x)
        out = self.relu(h[:, -1, :])  # láº¥y hidden cuá»‘i
        logits = self.fc(out)
        return logits, {"hidden": h, "activation": out}

model = InterpretableLSTM(input_dim, hidden_dim, num_classes)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=1e-3)

# -----------------------------
# 4ï¸âƒ£ TRAIN LOOP + LOG
# -----------------------------
for epoch in range(epochs):
    model.train()
    total_loss, correct = 0, 0

    for batch_X, batch_y in loader:
        optimizer.zero_grad()
        logits, internals = model(batch_X)
        loss = criterion(logits, batch_y)
        loss.backward()

        optimizer.step()

        total_loss += loss.item()
        correct += (logits.argmax(1) == batch_y).sum().item()

        # Log histograms
        for name, param in model.named_parameters():
            writer.add_histogram(f"weights/{name}", param, epoch)
            if param.grad is not None:
                writer.add_histogram(f"grads/{name}", param.grad, epoch)

        # Log activation stats
        writer.add_histogram("activations/last_hidden", internals["activation"], epoch)

    acc = correct / len(dataset)
    avg_loss = total_loss / len(loader)
    writer.add_scalar("Loss/train", avg_loss, epoch)
    writer.add_scalar("Accuracy/train", acc, epoch)

    print(f"Epoch {epoch+1}/{epochs} | Loss: {avg_loss:.4f} | Acc: {acc:.3f}")

# -----------------------------
# 5ï¸âƒ£ EMBEDDING PROJECTOR
# -----------------------------
embeddings = []
labels = []
with torch.no_grad():
    for Xb, yb in loader:
        logits, internals = model(Xb)
        embeddings.append(internals["activation"])
        labels.append(yb)
embeddings = torch.cat(embeddings)
labels = torch.cat(labels)
writer.add_embedding(embeddings, metadata=labels, tag="embedding_space")

# -----------------------------
# 6ï¸âƒ£ FEATURE IMPORTANCE (Captum)
# -----------------------------
ig = IntegratedGradients(model)
model.eval()
sample_X = X[0:1]
sample_y = y[0:1]
attr, delta = ig.attribute(sample_X, target=sample_y.item(), return_convergence_delta=True)

# Váº½ heatmap thá»ƒ hiá»‡n importance theo chiá»u thá»i gian + feature
attr_np = attr.squeeze(0).detach().numpy()
plt.figure(figsize=(8, 4))
plt.imshow(attr_np.T, cmap="hot", aspect="auto")
plt.colorbar(label="Feature importance")
plt.title("Integrated Gradients Heatmap (Captum)")
plt.xlabel("Time step")
plt.ylabel("Feature dim")
plt.tight_layout()
plt.savefig(os.path.join(log_dir, "feature_importance_heatmap.png"))
plt.close()

writer.close()
print("âœ… Training done. Logs saved to:", log_dir)
print("Run TensorBoard: tensorboard --logdir runs/model_interpret_full")









import torch
import matplotlib.pyplot as plt

# ============================================================
# 1ï¸âƒ£ Táº¡o ma tráº­n HiPPO-LegS (A, B)
# ============================================================
def hippo_legs_matrix(N: int):
    """Táº¡o ma tráº­n A, B cho HiPPO-LegS"""
    n = torch.arange(N, dtype=torch.float32)
    A = torch.zeros((N, N))
    for i in range(N):
        for j in range(N):
            if j <= i:
                A[i, j] = -(2 * i + 1) * ((-1) ** (i - j))
    B = (2 * n + 1).sqrt().unsqueeze(-1)
    return A, B

# ============================================================
# 2ï¸âƒ£ Rá»i ráº¡c hÃ³a báº±ng Bilinear transform (Tustin)
# ============================================================
def bilinear_discretize(A, B, dt=0.01):
    I = torch.eye(A.size(0))
    Ad = (I + 0.5 * dt * A) @ torch.linalg.inv(I - 0.5 * dt * A)
    Bd = torch.linalg.inv(I - 0.5 * dt * A) @ B * dt
    return Ad, Bd

# ============================================================
# 3ï¸âƒ£ MÃ´ phá»ng HiPPO vá»›i tÃ­n hiá»‡u x(t) = t
# ============================================================
def simulate_hippo(N=4, T=1.0, dt=0.01):
    steps = int(T / dt)
    A, B = hippo_legs_matrix(N)
    Ad, Bd = bilinear_discretize(A, B, dt)

    c = torch.zeros(N, 1)
    states = [c.clone()]
    xs = []
    ts = []

    for step in range(steps):
        t = step * dt
        x_t = torch.tensor([t])  # x(t) = t
        c = Ad @ c + Bd * x_t
        states.append(c.clone())
        xs.append(x_t.item())
        ts.append(t)

    states = torch.cat(states, dim=1)
    return ts, xs, states

# ============================================================
# 4ï¸âƒ£ Váº½ káº¿t quáº£
# ============================================================
ts, xs, states = simulate_hippo(N=4, T=1.0, dt=0.01)

plt.figure(figsize=(10, 6))
for i in range(states.size(0)):
    plt.plot(ts, states[i, 1:].numpy(), label=f'c[{i}]')

plt.title("Tiáº¿n hÃ³a há»‡ sá»‘ HiPPO-LegS cho x(t)=t")
plt.xlabel("Thá»i gian t")
plt.ylabel("GiÃ¡ trá»‹ há»‡ sá»‘ c_i(t)")
plt.legend()
plt.grid(True)
plt.show()





import pandas as pd
import numpy as np

INPUT = "NF-ToN-IoT-v2_1M_balanced.csv"
OUTPUT = "NF-ToN-IoT-v2_1M_balanced_shuffled.csv"
CHUNK_SIZE = 100_000
RANDOM_STATE = 1234

# 1. Äá»c file thÃ nh tá»«ng khá»‘i
chunks = pd.read_csv(INPUT, chunksize=CHUNK_SIZE)

# 2. Shuffle thá»© tá»± cÃ¡c khá»‘i
chunks = list(chunks)
np.random.seed(RANDOM_STATE)
np.random.shuffle(chunks)

# 3. Shuffle trong tá»«ng khá»‘i (Ã­t tá»‘n RAM)
for i, chunk in enumerate(chunks):
    chunk = chunk.sample(frac=1.0, random_state=RANDOM_STATE + i)
    mode = "w" if i == 0 else "a"
    header = (i == 0)
    chunk.to_csv(OUTPUT, index=False, mode=mode, header=header)

print("âœ… ÄÃ£ shuffle toÃ n bá»™ vÃ  lÆ°u:", OUTPUT)




import pandas as pd
import numpy as np

# ==============================================================
# âš™ï¸ Cáº¤U HÃŒNH
# ==============================================================
INPUT = "NF-ToN-IoT-v2_100k.csv"           # file Ä‘áº§u vÃ o
OUTPUT = "NF-ToN-IoT-v2_100k_clean.csv"    # file sau khi lÃ m sáº¡ch
CLIP_MIN, CLIP_MAX = -1e6, 1e6             # giá»›i háº¡n giÃ¡ trá»‹ cá»±c trá»‹

# ==============================================================
# 1ï¸âƒ£ Äá»c dá»¯ liá»‡u
# ==============================================================
df = pd.read_csv(INPUT)
print("ðŸ“Š KÃ­ch thÆ°á»›c ban Ä‘áº§u:", df.shape)

if 'Attack' not in df.columns:
    raise ValueError("âŒ KhÃ´ng tÃ¬m tháº¥y cá»™t 'Attack' trong file CSV.")

# ==============================================================
# 2ï¸âƒ£ Xá»­ lÃ½ NaN
# ==============================================================
# XoÃ¡ dÃ²ng khÃ´ng cÃ³ nhÃ£n Attack
df = df.dropna(subset=['Attack']).reset_index(drop=True)

# XÃ¡c Ä‘á»‹nh cá»™t sá»‘ vÃ  cá»™t chuá»—i
num_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()
cat_cols = df.select_dtypes(include=['object']).columns.drop('Attack').tolist()

# Äiá»n giÃ¡ trá»‹ thiáº¿u
for col in num_cols:
    df[col] = df[col].fillna(df[col].median())

for col in cat_cols:
    df[col] = df[col].fillna(df[col].mode()[0])

print(f"âœ… ÄÃ£ xá»­ lÃ½ NaN: {len(num_cols)} cá»™t sá»‘, {len(cat_cols)} cá»™t chuá»—i.")

# ==============================================================
# 3ï¸âƒ£ Giá»›i háº¡n (clip) giÃ¡ trá»‹ cá»±c trá»‹
# ==============================================================
df[num_cols] = df[num_cols].clip(lower=CLIP_MIN, upper=CLIP_MAX)
print(f"âœ… ÄÃ£ giá»›i háº¡n giÃ¡ trá»‹ trong khoáº£ng [{CLIP_MIN}, {CLIP_MAX}]")

# ==============================================================
# 4ï¸âƒ£ LÆ°u dá»¯ liá»‡u sáº¡ch
# ==============================================================
df.to_csv(OUTPUT, index=False)
print("âœ… ÄÃ£ lÆ°u file lÃ m sáº¡ch:", OUTPUT)





import pandas as pd
from sklearn.utils import resample

INPUT = "NF-ToN-IoT-v2.csv"
OUTPUT = "NF-ToN-IoT-v2_1M_balanced.csv"
TARGET_TOTAL = 1_000_000
RANDOM_STATE = 1234

df = pd.read_csv(INPUT)

if 'Attack' not in df.columns:
    raise ValueError("File CSV khÃ´ng cÃ³ cá»™t 'Attack'")

classes = df['Attack'].unique()
num_classes = len(classes)
target_per_class = TARGET_TOTAL // num_classes
remainder = TARGET_TOTAL % num_classes

sampled_parts = []
for i, cls in enumerate(sorted(classes)):
    cls_df = df[df['Attack'] == cls]
    this_target = target_per_class + (1 if i < remainder else 0)
    part = cls_df.sample(
        n=this_target,
        replace=(len(cls_df) < this_target),
        random_state=RANDOM_STATE,
    ).sample(frac=1.0, random_state=RANDOM_STATE)  # shuffle riÃªng tá»«ng lá»›p
    sampled_parts.append(part)

# concat mÃ  khÃ´ng shuffle toÃ n cá»¥c
df_balanced = pd.concat(sampled_parts, ignore_index=True)

print(df_balanced['Attack'].value_counts())

df_balanced.to_csv(OUTPUT, index=False)
print("Saved to:", OUTPUT)





