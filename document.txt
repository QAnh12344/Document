# train_whisper_v3.py
import os
import torch
import numpy as np
import soundfile as sf
from datasets import load_dataset
from transformers import (
    WhisperForConditionalGeneration,
    WhisperProcessor,
    Seq2SeqTrainingArguments,
    Seq2SeqTrainer
)
import evaluate
from dataclasses import dataclass
from typing import Any, Dict, List

# --- config ----------------------------------------------------------------------------
MODEL = "openai/whisper-large-v3"
DATASET_CSV = "data/manifest.csv"       # columns: path,text
OUTPUT_DIR = "whisper_ft_v3"
BATCH_SIZE = 4
EPOCHS = 3
LEARNING_RATE = 1e-5
# ---------------------------------------------------------------------------------------

processor = WhisperProcessor.from_pretrained(MODEL)
model = WhisperForConditionalGeneration.from_pretrained(MODEL)

# set language + task â†’ Báº®T BUá»˜C
model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(
    language="vi", task="transcribe"
)
model.config.suppress_tokens = []

model.to("cuda")

# load dataset
ds = load_dataset("csv", data_files={"train": DATASET_CSV})["train"]

# load audio
def load_audio(ex):
    speech, sr = sf.read(ex["path"])
    ex["audio"] = {"array": speech, "sampling_rate": sr}
    return ex

ds = ds.map(load_audio)

# preprocess fn
def preprocess(batch):
    audio = batch["audio"]

    # Whisper yÃªu cáº§u 16kHz
    if audio["sampling_rate"] != 16000:
        raise ValueError("Audio must be 16kHz. Resample first!")

    # extract log-mel
    features = processor(
        audio=audio["array"],
        sampling_rate=16000,
        return_tensors="pt"
    ).input_features[0]

    # tokenize text
    with processor.as_target_processor():
        labels = processor(batch["text"]).input_ids

    return {"input_features": features, "labels": labels}

ds = ds.map(preprocess, remove_columns=ds.column_names)

# collator
@dataclass
class DataCollatorSpeechSeq2Seq:
    processor: WhisperProcessor
    def __call__(self, features):
        input_features = torch.stack([torch.tensor(f["input_features"]) for f in features])
        labels = [f["labels"] for f in features]

        labels_batch = self.processor.tokenizer.pad(
            {"input_ids": labels}, 
            padding=True, 
            return_tensors="pt"
        ).input_ids

        labels_batch[labels_batch == self.processor.tokenizer.pad_token_id] = -100

        return {
            "input_features": input_features,
            "labels": labels_batch
        }

data_collator = DataCollatorSpeechSeq2Seq(processor)

# training args
training_args = Seq2SeqTrainingArguments(
    output_dir=OUTPUT_DIR,
    per_device_train_batch_size=BATCH_SIZE,
    gradient_accumulation_steps=4,
    fp16=True,
    num_train_epochs=EPOCHS,
    learning_rate=LEARNING_RATE,
    logging_steps=50,
    save_total_limit=2,
    evaluation_strategy="no",
    remove_unused_columns=False
)

# WER metric
wer = evaluate.load("wer")

def compute_metrics(pred):
    pred_ids = pred.predictions
    label_ids = pred.label_ids

    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id

    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)
    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)

    return {"wer": wer.compute(predictions=pred_str, references=label_str)}

# trainer
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=ds,
    data_collator=data_collator,
    tokenizer=processor.feature_extractor,
    compute_metrics=compute_metrics,
)

if __name__ == "__main__":
    trainer.train()
    trainer.save_model(OUTPUT_DIR)





import torch
from torch import nn
import inspect
from torch_geometric.nn.aggr import Aggregation
from torch_geometric.nn import MessagePassing
from torch_geometric.data import Data

# ---------------- MLPAutoencoder ----------------
class MLPAutoencoder(nn.Module):
    """
    Simple MLP Autoencoder: encode â†’ latent â†’ decode
    """
    def __init__(self, layer_sizes=(1, 2, 2, 4)):
        super().__init__()
        # Encoder
        enc_layers = []
        for i in range(len(layer_sizes) - 1):
            enc_layers.append(nn.Linear(layer_sizes[i], layer_sizes[i+1]))
            enc_layers.append(nn.ReLU())
        self.encoder = nn.Sequential(*enc_layers[:-1])  # remove last ReLU

        # Decoder (mirror of encoder)
        dec_layers = []
        rev_sizes = list(layer_sizes[::-1])
        for i in range(len(rev_sizes) - 1):
            dec_layers.append(nn.Linear(rev_sizes[i], rev_sizes[i+1]))
            dec_layers.append(nn.ReLU())
        self.decoder = nn.Sequential(*dec_layers[:-1])

    def forward(self, x):
        return self.encoder(x)

    def inverse(self, z):
        return self.decoder(z)

    def reset_parameters(self):
        for layer in self.encoder:
            if isinstance(layer, nn.Linear):
                nn.init.xavier_uniform_(layer.weight)
                nn.init.zeros_(layer.bias)
        for layer in self.decoder:
            if isinstance(layer, nn.Linear):
                nn.init.xavier_uniform_(layer.weight)
                nn.init.zeros_(layer.bias)


# ---------------- GenAgg ----------------
class GenAgg(Aggregation):
    """
    Generalized Aggregation using learnable MLP Autoencoder
    """
    def __init__(self, f=None, a=None, b=None, **kwargs):
        super().__init__()
        if f is None:
            self.f = MLPAutoencoder(layer_sizes=(1, 2, 2, 4))
        elif inspect.isclass(f):
            self.f = f(**kwargs)
        else:
            self.f = f

        self.a = nn.Parameter(torch.tensor(0.0)) if a is None else a
        self.b = nn.Parameter(torch.tensor(0.0)) if b is None else b

    def reset_parameters(self):
        nn.init.zeros_(self.a)
        nn.init.zeros_(self.b)
        if hasattr(self.f, 'reset_parameters'):
            self.f.reset_parameters()

    def get_n(self, x, index=None, ptr=None, dim_size=None, dim=-2):
        n = self.reduce(torch.ones_like(x), index, ptr, dim_size, dim, reduce='sum')
        n[n==0] = 1
        return n

    def forward(self, x, index=None, ptr=None, dim_size=None, dim=-2):
        if isinstance(self.b, nn.Parameter) or self.b != 0:
            x_mean = self.reduce(x, index, ptr, dim_size, dim, reduce='mean')
            if index is None:
                x = x - self.b * x_mean.unsqueeze(dim)
            else:
                x = x - self.b * torch.index_select(input=x_mean, dim=dim, index=index)

        n = self.get_n(x=x, index=index, ptr=ptr, dim_size=dim_size, dim=dim)
        x = x.unsqueeze(-1)
        n = n.unsqueeze(-1)
        if dim < 0:
            dim -= 1

        y1 = self.f.forward(x)
        y2 = self.reduce(y1, index, ptr, dim_size, dim, reduce='mean')
        y3 = y2 * (n**self.a)
        z = self.f.inverse(y3)
        z = z.squeeze(-1)

        return z

    def dist_op(self, a, b, type=1):
        if type == 1:
            return self.f.inverse(self.f.forward(a) * self.f.forward(b))
        elif type == 0:
            return self.f.inverse(self.f.forward(a) + self.f.forward(b))


# ---------------- Example: Message Passing GNN using GenAgg ----------------
class GenAggGNN(MessagePassing):
    def __init__(self, in_channels, out_channels):
        super().__init__(aggr=GenAgg())  # dÃ¹ng GenAgg aggregator
        self.lin = nn.Linear(in_channels, out_channels)

    def forward(self, x, edge_index):
        out = self.propagate(edge_index, x=x)
        out = self.lin(out)
        return out

    def message(self, x_j):
        return x_j

    def update(self, aggr_out):
        return aggr_out


# ---------------- Example usage ----------------
if __name__ == "__main__":
    # Create a small graph: 3 nodes, 4 edges
    edge_index = torch.tensor([[0, 1, 2, 0],
                               [1, 0, 0, 2]], dtype=torch.long)

    x = torch.randn(3, 5)  # 3 nodes, 5 features
    data = Data(x=x, edge_index=edge_index)

    # Initialize GNN
    model = GenAggGNN(in_channels=5, out_channels=4)

    # Forward
    out = model(data.x, data.edge_index)
    print("Output node features:", out)
    print("Shape:", out.shape)






import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader
from torch.utils.tensorboard import SummaryWriter
from captum.attr import IntegratedGradients
import matplotlib.pyplot as plt
import numpy as np
import random, os

# -----------------------------
# 1ï¸âƒ£ SETUP
# -----------------------------
torch.manual_seed(42)
random.seed(42)
np.random.seed(42)

input_dim = 20
hidden_dim = 32
num_classes = 3
num_samples = 500
batch_size = 32
epochs = 15
log_dir = "runs/model_interpret_full"

os.makedirs(log_dir, exist_ok=True)
writer = SummaryWriter(log_dir=log_dir)

# -----------------------------
# 2ï¸âƒ£ DATA: random cho dá»… nhÃ¬n
# -----------------------------
X = torch.randn(num_samples, 10, input_dim)  # (batch, seq, features)
y = torch.randint(0, num_classes, (num_samples,))

dataset = TensorDataset(X, y)
loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

# -----------------------------
# 3ï¸âƒ£ MODEL: LSTM cÆ¡ báº£n
# -----------------------------
class InterpretableLSTM(nn.Module):
    def __init__(self, input_dim, hidden_dim, num_classes):
        super().__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, num_classes)
        self.relu = nn.ReLU()

    def forward(self, x):
        h, _ = self.lstm(x)
        out = self.relu(h[:, -1, :])  # láº¥y hidden cuá»‘i
        logits = self.fc(out)
        return logits, {"hidden": h, "activation": out}

model = InterpretableLSTM(input_dim, hidden_dim, num_classes)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=1e-3)

# -----------------------------
# 4ï¸âƒ£ TRAIN LOOP + LOG
# -----------------------------
for epoch in range(epochs):
    model.train()
    total_loss, correct = 0, 0

    for batch_X, batch_y in loader:
        optimizer.zero_grad()
        logits, internals = model(batch_X)
        loss = criterion(logits, batch_y)
        loss.backward()

        optimizer.step()

        total_loss += loss.item()
        correct += (logits.argmax(1) == batch_y).sum().item()

        # Log histograms
        for name, param in model.named_parameters():
            writer.add_histogram(f"weights/{name}", param, epoch)
            if param.grad is not None:
                writer.add_histogram(f"grads/{name}", param.grad, epoch)

        # Log activation stats
        writer.add_histogram("activations/last_hidden", internals["activation"], epoch)

    acc = correct / len(dataset)
    avg_loss = total_loss / len(loader)
    writer.add_scalar("Loss/train", avg_loss, epoch)
    writer.add_scalar("Accuracy/train", acc, epoch)

    print(f"Epoch {epoch+1}/{epochs} | Loss: {avg_loss:.4f} | Acc: {acc:.3f}")

# -----------------------------
# 5ï¸âƒ£ EMBEDDING PROJECTOR
# -----------------------------
embeddings = []
labels = []
with torch.no_grad():
    for Xb, yb in loader:
        logits, internals = model(Xb)
        embeddings.append(internals["activation"])
        labels.append(yb)
embeddings = torch.cat(embeddings)
labels = torch.cat(labels)
writer.add_embedding(embeddings, metadata=labels, tag="embedding_space")

# -----------------------------
# 6ï¸âƒ£ FEATURE IMPORTANCE (Captum)
# -----------------------------
ig = IntegratedGradients(model)
model.eval()
sample_X = X[0:1]
sample_y = y[0:1]
attr, delta = ig.attribute(sample_X, target=sample_y.item(), return_convergence_delta=True)

# Váº½ heatmap thá»ƒ hiá»‡n importance theo chiá»u thá»i gian + feature
attr_np = attr.squeeze(0).detach().numpy()
plt.figure(figsize=(8, 4))
plt.imshow(attr_np.T, cmap="hot", aspect="auto")
plt.colorbar(label="Feature importance")
plt.title("Integrated Gradients Heatmap (Captum)")
plt.xlabel("Time step")
plt.ylabel("Feature dim")
plt.tight_layout()
plt.savefig(os.path.join(log_dir, "feature_importance_heatmap.png"))
plt.close()

writer.close()
print("âœ… Training done. Logs saved to:", log_dir)
print("Run TensorBoard: tensorboard --logdir runs/model_interpret_full")









import torch
import matplotlib.pyplot as plt

# ============================================================
# 1ï¸âƒ£ Táº¡o ma tráº­n HiPPO-LegS (A, B)
# ============================================================
def hippo_legs_matrix(N: int):
    """Táº¡o ma tráº­n A, B cho HiPPO-LegS"""
    n = torch.arange(N, dtype=torch.float32)
    A = torch.zeros((N, N))
    for i in range(N):
        for j in range(N):
            if j <= i:
                A[i, j] = -(2 * i + 1) * ((-1) ** (i - j))
    B = (2 * n + 1).sqrt().unsqueeze(-1)
    return A, B

# ============================================================
# 2ï¸âƒ£ Rá»i ráº¡c hÃ³a báº±ng Bilinear transform (Tustin)
# ============================================================
def bilinear_discretize(A, B, dt=0.01):
    I = torch.eye(A.size(0))
    Ad = (I + 0.5 * dt * A) @ torch.linalg.inv(I - 0.5 * dt * A)
    Bd = torch.linalg.inv(I - 0.5 * dt * A) @ B * dt
    return Ad, Bd

# ============================================================
# 3ï¸âƒ£ MÃ´ phá»ng HiPPO vá»›i tÃ­n hiá»‡u x(t) = t
# ============================================================
def simulate_hippo(N=4, T=1.0, dt=0.01):
    steps = int(T / dt)
    A, B = hippo_legs_matrix(N)
    Ad, Bd = bilinear_discretize(A, B, dt)

    c = torch.zeros(N, 1)
    states = [c.clone()]
    xs = []
    ts = []

    for step in range(steps):
        t = step * dt
        x_t = torch.tensor([t])  # x(t) = t
        c = Ad @ c + Bd * x_t
        states.append(c.clone())
        xs.append(x_t.item())
        ts.append(t)

    states = torch.cat(states, dim=1)
    return ts, xs, states

# ============================================================
# 4ï¸âƒ£ Váº½ káº¿t quáº£
# ============================================================
ts, xs, states = simulate_hippo(N=4, T=1.0, dt=0.01)

plt.figure(figsize=(10, 6))
for i in range(states.size(0)):
    plt.plot(ts, states[i, 1:].numpy(), label=f'c[{i}]')

plt.title("Tiáº¿n hÃ³a há»‡ sá»‘ HiPPO-LegS cho x(t)=t")
plt.xlabel("Thá»i gian t")
plt.ylabel("GiÃ¡ trá»‹ há»‡ sá»‘ c_i(t)")
plt.legend()
plt.grid(True)
plt.show()





import pandas as pd
import numpy as np

INPUT = "NF-ToN-IoT-v2_1M_balanced.csv"
OUTPUT = "NF-ToN-IoT-v2_1M_balanced_shuffled.csv"
CHUNK_SIZE = 100_000
RANDOM_STATE = 1234

# 1. Äá»c file thÃ nh tá»«ng khá»‘i
chunks = pd.read_csv(INPUT, chunksize=CHUNK_SIZE)

# 2. Shuffle thá»© tá»± cÃ¡c khá»‘i
chunks = list(chunks)
np.random.seed(RANDOM_STATE)
np.random.shuffle(chunks)

# 3. Shuffle trong tá»«ng khá»‘i (Ã­t tá»‘n RAM)
for i, chunk in enumerate(chunks):
    chunk = chunk.sample(frac=1.0, random_state=RANDOM_STATE + i)
    mode = "w" if i == 0 else "a"
    header = (i == 0)
    chunk.to_csv(OUTPUT, index=False, mode=mode, header=header)

print("âœ… ÄÃ£ shuffle toÃ n bá»™ vÃ  lÆ°u:", OUTPUT)




import pandas as pd
import numpy as np

# ==============================================================
# âš™ï¸ Cáº¤U HÃŒNH
# ==============================================================
INPUT = "NF-ToN-IoT-v2_100k.csv"           # file Ä‘áº§u vÃ o
OUTPUT = "NF-ToN-IoT-v2_100k_clean.csv"    # file sau khi lÃ m sáº¡ch
CLIP_MIN, CLIP_MAX = -1e6, 1e6             # giá»›i háº¡n giÃ¡ trá»‹ cá»±c trá»‹

# ==============================================================
# 1ï¸âƒ£ Äá»c dá»¯ liá»‡u
# ==============================================================
df = pd.read_csv(INPUT)
print("ðŸ“Š KÃ­ch thÆ°á»›c ban Ä‘áº§u:", df.shape)

if 'Attack' not in df.columns:
    raise ValueError("âŒ KhÃ´ng tÃ¬m tháº¥y cá»™t 'Attack' trong file CSV.")

# ==============================================================
# 2ï¸âƒ£ Xá»­ lÃ½ NaN
# ==============================================================
# XoÃ¡ dÃ²ng khÃ´ng cÃ³ nhÃ£n Attack
df = df.dropna(subset=['Attack']).reset_index(drop=True)

# XÃ¡c Ä‘á»‹nh cá»™t sá»‘ vÃ  cá»™t chuá»—i
num_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()
cat_cols = df.select_dtypes(include=['object']).columns.drop('Attack').tolist()

# Äiá»n giÃ¡ trá»‹ thiáº¿u
for col in num_cols:
    df[col] = df[col].fillna(df[col].median())

for col in cat_cols:
    df[col] = df[col].fillna(df[col].mode()[0])

print(f"âœ… ÄÃ£ xá»­ lÃ½ NaN: {len(num_cols)} cá»™t sá»‘, {len(cat_cols)} cá»™t chuá»—i.")

# ==============================================================
# 3ï¸âƒ£ Giá»›i háº¡n (clip) giÃ¡ trá»‹ cá»±c trá»‹
# ==============================================================
df[num_cols] = df[num_cols].clip(lower=CLIP_MIN, upper=CLIP_MAX)
print(f"âœ… ÄÃ£ giá»›i háº¡n giÃ¡ trá»‹ trong khoáº£ng [{CLIP_MIN}, {CLIP_MAX}]")

# ==============================================================
# 4ï¸âƒ£ LÆ°u dá»¯ liá»‡u sáº¡ch
# ==============================================================
df.to_csv(OUTPUT, index=False)
print("âœ… ÄÃ£ lÆ°u file lÃ m sáº¡ch:", OUTPUT)





import pandas as pd
from sklearn.utils import resample

INPUT = "NF-ToN-IoT-v2.csv"
OUTPUT = "NF-ToN-IoT-v2_1M_balanced.csv"
TARGET_TOTAL = 1_000_000
RANDOM_STATE = 1234

df = pd.read_csv(INPUT)

if 'Attack' not in df.columns:
    raise ValueError("File CSV khÃ´ng cÃ³ cá»™t 'Attack'")

classes = df['Attack'].unique()
num_classes = len(classes)
target_per_class = TARGET_TOTAL // num_classes
remainder = TARGET_TOTAL % num_classes

sampled_parts = []
for i, cls in enumerate(sorted(classes)):
    cls_df = df[df['Attack'] == cls]
    this_target = target_per_class + (1 if i < remainder else 0)
    part = cls_df.sample(
        n=this_target,
        replace=(len(cls_df) < this_target),
        random_state=RANDOM_STATE,
    ).sample(frac=1.0, random_state=RANDOM_STATE)  # shuffle riÃªng tá»«ng lá»›p
    sampled_parts.append(part)

# concat mÃ  khÃ´ng shuffle toÃ n cá»¥c
df_balanced = pd.concat(sampled_parts, ignore_index=True)

print(df_balanced['Attack'].value_counts())

df_balanced.to_csv(OUTPUT, index=False)
print("Saved to:", OUTPUT)





